{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48d4cc45-7866-4110-abe0-7ec7b15522f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "tel = nn.TransformerEncoderLayer(d_model=12,nhead=3,batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d4afac8-630a-46c6-ace6-27d109dc6e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_proj_weight: torch.Size([36, 12])\n",
      "in_proj_bias: torch.Size([36])\n",
      "out_proj.weight: torch.Size([12, 12])\n",
      "out_proj.bias: torch.Size([12])\n"
     ]
    }
   ],
   "source": [
    "for name,param in tel.self_attn.named_parameters():\n",
    "    print(f'{name}: {param.shape}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0b5dfafd-6294-4ce6-b8dd-fa77bbdf467b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.MultiheadAttention(embed_dim=12,num_heads=3).out_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822180a0-fe1a-417f-8c7f-c62b2fe27440",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.MultiheadAttention(embed_dim=12,num_heads=3).out_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cb4d48-bfc9-400e-afd8-0a61bfc5030e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.MultiheadAttention(embed_dim=12,num_heads=3).out_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03d532e2-add8-4490-ad6b-c86d740958a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerEncoderLayer(\n",
       "  (self_attn): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "  )\n",
       "  (linear1): Linear(in_features=12, out_features=2048, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (linear2): Linear(in_features=2048, out_features=12, bias=True)\n",
       "  (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "  (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "  (dropout1): Dropout(p=0.1, inplace=False)\n",
       "  (dropout2): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9661012f-61c1-449e-ad3c-960d7017422b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "TransformerEncoderLayer                  --                        --\n",
       "├─MultiheadAttention: 1-1                [5, 3, 12]                --\n",
       "├─Dropout: 1-2                           [5, 3, 12]                --\n",
       "├─LayerNorm: 1-3                         [5, 3, 12]                24\n",
       "├─Linear: 1-4                            [5, 3, 2048]              26,624\n",
       "├─Dropout: 1-5                           [5, 3, 2048]              --\n",
       "├─Linear: 1-6                            [5, 3, 12]                24,588\n",
       "├─Dropout: 1-7                           [5, 3, 12]                --\n",
       "├─LayerNorm: 1-8                         [5, 3, 12]                24\n",
       "==========================================================================================\n",
       "Total params: 51,260\n",
       "Trainable params: 51,260\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.26\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.25\n",
       "Params size (MB): 0.21\n",
       "Estimated Total Size (MB): 0.46\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary as sm\n",
    "sm(tel,(5,3,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "429a69da-1cca-4fb7-9c7a-6c13fe06706a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 12])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tel(torch.rand(5,3,12)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed63870b-c631-44ea-a1cb-dcf12282fc95",
   "metadata": {},
   "source": [
    "### external attentions  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8699bf75-b555-4266-b58b-be866a3e26eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "ext_path = os.path.abspath('./ext_attns/')\n",
    "\n",
    "import sys\n",
    "sys.path.append(ext_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "87a009dc-5b06-445a-aa39-31bb78faad25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 49, 512])\n"
     ]
    }
   ],
   "source": [
    "# 1.external attn: \"Beyond Self-attention: External Attention using Two Linear Layers for Visual Tasks\"\n",
    "from model.attention.ExternalAttention import ExternalAttention\n",
    "import torch\n",
    "\n",
    "input=torch.randn(50,49,512)\n",
    "ea = ExternalAttention(d_model=512,S=8)\n",
    "output=ea(input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9c474ad6-3cf0-46b5-ac42-1ac366d5f80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 49, 512])\n"
     ]
    }
   ],
   "source": [
    "# 2. self attn: attention is all you need \n",
    "from model.attention.SelfAttention import ScaledDotProductAttention\n",
    "import torch\n",
    "\n",
    "input=torch.randn(50,49,512)\n",
    "sa = ScaledDotProductAttention(d_model=512, d_k=512, d_v=512, h=8)\n",
    "output=sa(input,input,input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6cebd182-8c63-4f86-a670-bdd88036a7f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 49, 512])\n"
     ]
    }
   ],
   "source": [
    "# 3. simplified self attn: None\n",
    "from model.attention.SimplifiedSelfAttention import SimplifiedScaledDotProductAttention\n",
    "import torch\n",
    "\n",
    "input=torch.randn(50,49,512)\n",
    "ssa = SimplifiedScaledDotProductAttention(d_model=512, h=8)\n",
    "output=ssa(input,input,input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cf97f6bc-89ce-4e36-a6d3-396cc128b0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 64, 512])\n"
     ]
    }
   ],
   "source": [
    "# 10 Efficient Multi-Head Self-Attention: \"ResT: An Efficient Transformer for Visual Recognition\"\n",
    "from model.attention.EMSA import EMSA\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "input=torch.randn(50,64,512)\n",
    "emsa = EMSA(d_model=512, d_k=512, d_v=512, h=8,H=8,W=8,ratio=2,apply_transform=True)\n",
    "output=emsa(input,input,input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9c1fc043-0677-4cb9-a8c4-058d251fde06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 49, 512])\n"
     ]
    }
   ],
   "source": [
    "# 13 muse attn: \"MUSE: Parallel Multi-Scale Attention for Sequence to Sequence Learning\"\n",
    "from model.attention.MUSEAttention import MUSEAttention\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "input=torch.randn(50,49,512)\n",
    "sa = MUSEAttention(d_model=512, d_k=512, d_v=512, h=8)\n",
    "output=sa(input,input,input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "44ab9f18-2e0b-40d6-970e-a82e4fd23640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 49, 512])\n"
     ]
    }
   ],
   "source": [
    "# 16 aft: An Attention Free Transformer\n",
    "from model.attention.AFT import AFT_FULL\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "input=torch.randn(50,49,512)\n",
    "aft_full = AFT_FULL(d_model=512, n=49)\n",
    "output=aft_full(input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "07071453-26a8-4d53-af46-7e02bd0a5af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 49, 512])\n"
     ]
    }
   ],
   "source": [
    "# 30 UFO: UFO-ViT: High Performance Linear Vision Transformer without Softmax---ArXiv 2021.09.29\n",
    "from model.attention.UFOAttention import *\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "input=torch.randn(50,49,512)\n",
    "ufo = UFOAttention(d_model=512, d_k=512, d_v=512, h=8)\n",
    "output=ufo(input,input,input)\n",
    "print(output.shape) #[50, 49, 512]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f63dd53-bdbf-43a4-b275-8e23f09534c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### importing test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d1af3b3c-795d-4281-9d6a-2ce1c19f2c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y:torch.Size([3, 4, 768]) pre:torch.Size([3, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "# assertion test\n",
    "x = torch.rand(3, 4, 768)\n",
    "y = torch.rand(3, 4, 768)\n",
    "# for attn in attn_li:\n",
    "# pre = attn_li[0](x,x,x)\n",
    "# pre = attn_li[1](x)     not solved\n",
    "# pre = attn_li[2](x,x,x)\n",
    "# pre = attn_li[3](x,x,x) not solved\n",
    "# pre = attn_li[4](x,x,x)\n",
    "# pre = attn_li[5](x,x,x)\n",
    "# pre = attn_li[6](x)\n",
    "assert y.shape == pre.shape\n",
    "print(f'y:{y.shape} pre:{pre.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "543b5356-e3ef-4897-a43e-76b1d5f22d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset xsum (/home/yp/.cache/huggingface/datasets/xsum/default/1.2.0/4957825a982999fbf80bca0b342793b01b2611e021ef589fb7c6250b3577b499)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca358c6a240e4ad5a3cb9a7c41334415",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/yp/.cache/huggingface/datasets/xsum/default/1.2.0/4957825a982999fbf80bca0b342793b01b2611e021ef589fb7c6250b3577b499/cache-5d732b1c86657ea0.arrow\n",
      "Loading cached processed dataset at /home/yp/.cache/huggingface/datasets/xsum/default/1.2.0/4957825a982999fbf80bca0b342793b01b2611e021ef589fb7c6250b3577b499/cache-38966a651c72a103.arrow\n",
      "Loading cached processed dataset at /home/yp/.cache/huggingface/datasets/xsum/default/1.2.0/4957825a982999fbf80bca0b342793b01b2611e021ef589fb7c6250b3577b499/cache-d19c279816291ade.arrow\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "ext_path = os.path.abspath('./ext_attns/')\n",
    "import sys\n",
    "sys.path.append(ext_path)\n",
    "# list\n",
    "from model.attention.UFOAttention import *\n",
    "from model.attention.AFT import AFT_FULL\n",
    "from model.attention.MUSEAttention import MUSEAttention\n",
    "from model.attention.EMSA import EMSA\n",
    "from model.attention.SimplifiedSelfAttention import SimplifiedScaledDotProductAttention\n",
    "from model.attention.SelfAttention import ScaledDotProductAttention\n",
    "from model.attention.ExternalAttention import ExternalAttention\n",
    "d_model = 768\n",
    "n_head  = 12\n",
    "ufo = UFOAttention(d_model=d_model, d_k=d_model//n_head, d_v=d_model//n_head, h=n_head)\n",
    "aft_full = AFT_FULL(d_model=d_model, n=n_head)\n",
    "ma = MUSEAttention(d_model=d_model, d_k=d_model//n_head, d_v=d_model//n_head, h=n_head)\n",
    "emsa = EMSA(d_model=d_model, d_k=d_model//n_head, d_v=d_model//n_head, h=n_head,H=n_head,W=n_head,ratio=2,apply_transform=True)\n",
    "ssa = SimplifiedScaledDotProductAttention(d_model=d_model, h=n_head)\n",
    "sa = ScaledDotProductAttention(d_model=d_model, d_k=d_model//n_head, d_v=d_model//n_head, h=n_head)\n",
    "ea = ExternalAttention(d_model=d_model,S=8) #v\n",
    "\n",
    "attn_li = [ufo,aft_full,ma,emsa,ssa,sa,ea]\n",
    "\n",
    "import transformers\n",
    "\n",
    "# print(transformers.__version__)\n",
    "\n",
    "model_checkpoint = \"t5-small\"\n",
    "model_checkpoint = 'facebook/bart-base'\n",
    "\n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "raw_datasets = load_dataset(\"xsum\")\n",
    "metric = load_metric(\"rouge\")\n",
    "\n",
    "# raw_datasets[\"train\"][0]\n",
    "\n",
    "import datasets\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# ?\n",
    "# with tokenizer.as_target_tokenizer():\n",
    "#     print(tokenizer([\"Hello, this one sentence!\", \"This is another sentence.\"]))\n",
    "\n",
    "if model_checkpoint in [\"t5-small\", \"t5-base\", \"t5-larg\", \"t5-3b\", \"t5-11b\"]:\n",
    "    prefix = \"summarize: \"\n",
    "else:\n",
    "    prefix = \"\"\n",
    "\n",
    "max_input_length = 1024\n",
    "max_target_length = 128\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"document\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"summary\"], max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "batch_size = 3 #16\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    f\"{model_name}-finetuned-xsum\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "#     push_to_hub=True,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Rouge expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    \n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    # Extract a few results\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    \n",
    "    # Add mean generated length\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    \n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffb46ac4-462a-4490-b851-31b989292953",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "for i,attn in enumerate(attn_li[:]):\n",
    "    not_allowed = ('AFT_FULL','EMSA')\n",
    "    print(attn)\n",
    "    if attn.__str__().startswith(not_allowed):\n",
    "        continue\n",
    "        \n",
    "    class tel2lin1(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(tel2lin1,self).__init__()\n",
    "            # self.tel = nn.TransformerEncoderLayer(d_model=768,nhead=3,batch_first=True)\n",
    "            self.tel = attn\n",
    "            self.lin = nn.Linear(in_features=768,out_features=50265, bias=False)\n",
    "            \n",
    "        def forward(self,x):\n",
    "            if self.tel.__str__().startswith('ExternalAttention'):\n",
    "                x = self.tel(x)\n",
    "            else:\n",
    "                x = self.tel(x,x,x)\n",
    "            x = self.lin(x)\n",
    "            return x\n",
    "    model.lm_head = tel2lin1()\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model,\n",
    "        args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"validation\"],\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd69593-51ec-45a8-9541-d46da81e1f3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
