{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48d4cc45-7866-4110-abe0-7ec7b15522f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "tel = nn.TransformerEncoderLayer(d_model=12,nhead=3,batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d4afac8-630a-46c6-ace6-27d109dc6e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_proj_weight: torch.Size([36, 12])\n",
      "in_proj_bias: torch.Size([36])\n",
      "out_proj.weight: torch.Size([12, 12])\n",
      "out_proj.bias: torch.Size([12])\n"
     ]
    }
   ],
   "source": [
    "for name,param in tel.self_attn.named_parameters():\n",
    "    print(f'{name}: {param.shape}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0b5dfafd-6294-4ce6-b8dd-fa77bbdf467b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.MultiheadAttention(embed_dim=12,num_heads=3).out_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822180a0-fe1a-417f-8c7f-c62b2fe27440",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.MultiheadAttention(embed_dim=12,num_heads=3).out_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cb4d48-bfc9-400e-afd8-0a61bfc5030e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.MultiheadAttention(embed_dim=12,num_heads=3).out_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03d532e2-add8-4490-ad6b-c86d740958a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerEncoderLayer(\n",
       "  (self_attn): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "  )\n",
       "  (linear1): Linear(in_features=12, out_features=2048, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (linear2): Linear(in_features=2048, out_features=12, bias=True)\n",
       "  (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "  (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "  (dropout1): Dropout(p=0.1, inplace=False)\n",
       "  (dropout2): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9661012f-61c1-449e-ad3c-960d7017422b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "TransformerEncoderLayer                  --                        --\n",
       "├─MultiheadAttention: 1-1                [5, 3, 12]                --\n",
       "├─Dropout: 1-2                           [5, 3, 12]                --\n",
       "├─LayerNorm: 1-3                         [5, 3, 12]                24\n",
       "├─Linear: 1-4                            [5, 3, 2048]              26,624\n",
       "├─Dropout: 1-5                           [5, 3, 2048]              --\n",
       "├─Linear: 1-6                            [5, 3, 12]                24,588\n",
       "├─Dropout: 1-7                           [5, 3, 12]                --\n",
       "├─LayerNorm: 1-8                         [5, 3, 12]                24\n",
       "==========================================================================================\n",
       "Total params: 51,260\n",
       "Trainable params: 51,260\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.26\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.25\n",
       "Params size (MB): 0.21\n",
       "Estimated Total Size (MB): 0.46\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary as sm\n",
    "sm(tel,(5,3,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "429a69da-1cca-4fb7-9c7a-6c13fe06706a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 12])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tel(torch.rand(5,3,12)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed63870b-c631-44ea-a1cb-dcf12282fc95",
   "metadata": {},
   "source": [
    "### external attentions  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ec8774db-c7c0-43fe-a35a-6b4896745fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "ext_path = os.path.abspath('./ext_attns/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4457704d-8682-4b22-b3bc-6a5c8b3575a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(ext_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2f415b1a-72c0-467e-9f35-052888bbfc71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/yp/workspace/nlp-sm/ext_attns'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.rand(5,5,21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a009dc-5b06-445a-aa39-31bb78faad25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.external attn: \"Beyond Self-attention: External Attention using Two Linear Layers for Visual Tasks\"\n",
    "from model.attention.ExternalAttention import ExternalAttention\n",
    "import torch\n",
    "\n",
    "input=torch.randn(50,49,512)\n",
    "ea = ExternalAttention(d_model=512,S=8)\n",
    "output=ea(input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c474ad6-3cf0-46b5-ac42-1ac366d5f80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. self attn: attention is all you need \n",
    "from model.attention.SelfAttention import ScaledDotProductAttention\n",
    "import torch\n",
    "\n",
    "input=torch.randn(50,49,512)\n",
    "sa = ScaledDotProductAttention(d_model=512, d_k=512, d_v=512, h=8)\n",
    "output=sa(input,input,input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cebd182-8c63-4f86-a670-bdd88036a7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. simplified self attn: None\n",
    "from model.attention.SimplifiedSelfAttention import SimplifiedScaledDotProductAttention\n",
    "import torch\n",
    "\n",
    "input=torch.randn(50,49,512)\n",
    "ssa = SimplifiedScaledDotProductAttention(d_model=512, h=8)\n",
    "output=ssa(input,input,input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf97f6bc-89ce-4e36-a6d3-396cc128b0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 Efficient Multi-Head Self-Attention: \"ResT: An Efficient Transformer for Visual Recognition\"\n",
    "from model.attention.EMSA import EMSA\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "input=torch.randn(50,64,512)\n",
    "emsa = EMSA(d_model=512, d_k=512, d_v=512, h=8,H=8,W=8,ratio=2,apply_transform=True)\n",
    "output=emsa(input,input,input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1fc043-0677-4cb9-a8c4-058d251fde06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13 muse attn: \"MUSE: Parallel Multi-Scale Attention for Sequence to Sequence Learning\"\n",
    "from model.attention.MUSEAttention import MUSEAttention\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "input=torch.randn(50,49,512)\n",
    "sa = MUSEAttention(d_model=512, d_k=512, d_v=512, h=8)\n",
    "output=sa(input,input,input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ab9f18-2e0b-40d6-970e-a82e4fd23640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16 aft: An Attention Free Transformer\n",
    "from model.attention.AFT import AFT_FULL\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "input=torch.randn(50,49,512)\n",
    "aft_full = AFT_FULL(d_model=512, n=49)\n",
    "output=aft_full(input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07071453-26a8-4d53-af46-7e02bd0a5af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30 UFO: UFO-ViT: High Performance Linear Vision Transformer without Softmax---ArXiv 2021.09.29\n",
    "from model.attention.UFOAttention import *\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    input=torch.randn(50,49,512)\n",
    "    ufo = UFOAttention(d_model=512, d_k=512, d_v=512, h=8)\n",
    "    output=ufo(input,input,input)\n",
    "    print(output.shape) #[50, 49, 512]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
