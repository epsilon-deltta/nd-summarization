{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5191c69-70d7-4b48-8958-a937e30e160d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "ext_path = os.path.abspath('./ext_attns/')\n",
    "import sys\n",
    "sys.path.append(ext_path)\n",
    "# list\n",
    "from model.attention.UFOAttention import *\n",
    "from model.attention.AFT import AFT_FULL\n",
    "from model.attention.MUSEAttention import MUSEAttention\n",
    "from model.attention.EMSA import EMSA\n",
    "from model.attention.SimplifiedSelfAttention import SimplifiedScaledDotProductAttention\n",
    "from model.attention.SelfAttention import ScaledDotProductAttention\n",
    "from model.attention.ExternalAttention import ExternalAttention\n",
    "d_model = 768 \n",
    "n_head  = 12  \n",
    "ufo = UFOAttention(d_model=d_model, d_k=d_model//n_head, d_v=d_model//n_head, h=n_head)\n",
    "aft_full = AFT_FULL(d_model=d_model, n=n_head)\n",
    "ma = MUSEAttention(d_model=d_model, d_k=d_model//n_head, d_v=d_model//n_head, h=n_head)\n",
    "emsa = EMSA(d_model=d_model, d_k=d_model//n_head, d_v=d_model//n_head, h=n_head,H=n_head,W=n_head,ratio=2,apply_transform=True)\n",
    "ssa = SimplifiedScaledDotProductAttention(d_model=d_model, h=n_head)\n",
    "sa = ScaledDotProductAttention(d_model=d_model, d_k=d_model//n_head, d_v=d_model//n_head, h=n_head)\n",
    "ea = ExternalAttention(d_model=d_model,S=8) #v\n",
    "\n",
    "attn_li = [ufo,aft_full,ma,emsa,ssa,sa,ea]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85975d0e-e206-4400-82dd-c819d8a6ac4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset xsum (/home/yp/.cache/huggingface/datasets/xsum/default/1.2.0/4957825a982999fbf80bca0b342793b01b2611e021ef589fb7c6250b3577b499)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57de11dff84b4ffa8de3f608c4484ee8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/yp/.cache/huggingface/datasets/xsum/default/1.2.0/4957825a982999fbf80bca0b342793b01b2611e021ef589fb7c6250b3577b499/cache-5d732b1c86657ea0.arrow\n",
      "Loading cached processed dataset at /home/yp/.cache/huggingface/datasets/xsum/default/1.2.0/4957825a982999fbf80bca0b342793b01b2611e021ef589fb7c6250b3577b499/cache-38966a651c72a103.arrow\n",
      "Loading cached processed dataset at /home/yp/.cache/huggingface/datasets/xsum/default/1.2.0/4957825a982999fbf80bca0b342793b01b2611e021ef589fb7c6250b3577b499/cache-d19c279816291ade.arrow\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "# print(transformers.__version__)\n",
    "\n",
    "model_checkpoint = \"t5-small\"\n",
    "model_checkpoint = 'facebook/bart-base'\n",
    "\n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "raw_datasets = load_dataset(\"xsum\")\n",
    "metric = load_metric(\"rouge\")\n",
    "\n",
    "# raw_datasets[\"train\"][0]\n",
    "\n",
    "import datasets\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# ?\n",
    "# with tokenizer.as_target_tokenizer():\n",
    "#     print(tokenizer([\"Hello, this one sentence!\", \"This is another sentence.\"]))\n",
    "\n",
    "if model_checkpoint in [\"t5-small\", \"t5-base\", \"t5-larg\", \"t5-3b\", \"t5-11b\"]:\n",
    "    prefix = \"summarize: \"\n",
    "else:\n",
    "    prefix = \"\"\n",
    "\n",
    "max_input_length = 1024\n",
    "max_target_length = 128\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"document\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"summary\"], max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "batch_size = 3 #16\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    f\"{model_name}-finetuned-xsum\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "#     push_to_hub=True,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Rouge expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    \n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    # Extract a few results\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    \n",
    "    # Add mean generated length\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    \n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04be5338-a180-4722-8577-a461439631ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "tr_max = 20000 #204045\n",
    "val_max = 1332 #11332\n",
    "for i,attn in enumerate(attn_li[:]):\n",
    "    not_allowed = ('AFT_FULL','EMSA')\n",
    "    print(attn)\n",
    "    if attn.__str__().startswith(not_allowed):\n",
    "        continue\n",
    "        \n",
    "    class tel2lin1(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(tel2lin1,self).__init__()\n",
    "            # self.tel = nn.TransformerEncoderLayer(d_model=768,nhead=3,batch_first=True)\n",
    "            self.tel = attn\n",
    "            self.lin = nn.Linear(in_features=768,out_features=50265, bias=False)\n",
    "            \n",
    "        def forward(self,x):\n",
    "            if self.tel.__str__().startswith('ExternalAttention'):\n",
    "                x = self.tel(x)\n",
    "            else:\n",
    "                x = self.tel(x,x,x)\n",
    "            x = self.lin(x)\n",
    "            return x\n",
    "    model.lm_head = tel2lin1()\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model,\n",
    "        args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"validation\"],\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39745406-4976-4762-b166-2fe29fe8d92e",
   "metadata": {},
   "source": [
    "### About dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53ea756c-8f5c-4258-b80a-47c2109c7c9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['attention_mask', 'document', 'id', 'input_ids', 'labels', 'summary'],\n",
       "        num_rows: 204045\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['attention_mask', 'document', 'id', 'input_ids', 'labels', 'summary'],\n",
       "        num_rows: 11332\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['attention_mask', 'document', 'id', 'input_ids', 'labels', 'summary'],\n",
       "        num_rows: 11334\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "828b258b-c091-4f2f-9f91-fda068d293f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 204045\n",
      "val:   11332\n",
      "test:  11334\n"
     ]
    }
   ],
   "source": [
    "print(f'train: {len(tokenized_datasets[\"train\"])}')\n",
    "print(f'val:   {len(tokenized_datasets[\"validation\"])}')\n",
    "print(f'test:  {len(tokenized_datasets[\"test\"])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50ab7ce0-b46d-4ea3-b8f6-205b8651beed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['attention_mask', 'document', 'id', 'input_ids', 'labels', 'summary'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_max=20000\n",
    "tokenized_datasets['train'][:tr_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "59906a6b-a093-4cd3-8dda-553244dc7955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['attention_mask', 'document', 'id', 'input_ids', 'labels', 'summary'],\n",
       "    num_rows: 204045\n",
       "})"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "73094c79-f85d-4f76-9c8f-6ac6317b37ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01960351883163028"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4000/204045"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce481b0-db3d-44c2-a62c-6927c523a9f5",
   "metadata": {},
   "source": [
    "### FT on small dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596aa51d-6b0b-4c8d-8316-8069c6329766",
   "metadata": {},
   "source": [
    "|i|attention | used|\n",
    "|---|--------| ----|\n",
    "|0  |UFOAttention| ✔|\n",
    "|1  | AFT_FULL   |x|\n",
    "|2  | MUSEAttention|✔ |\n",
    "|3  | EMSA|     x|\n",
    "|4  |SimplifiedScaledDotProductAttention|✔ |\n",
    "|5  |ScaledDotProductAttention|✔ |\n",
    "|6  |ExternalAttention|✔ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31a02343-6b97-41c9-89db-01f41a0151c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset xsum (/home/yp/.cache/huggingface/datasets/xsum/default/1.2.0/4957825a982999fbf80bca0b342793b01b2611e021ef589fb7c6250b3577b499)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "013a0071960543b2960aeb1f45520452",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/yp/.cache/huggingface/datasets/xsum/default/1.2.0/4957825a982999fbf80bca0b342793b01b2611e021ef589fb7c6250b3577b499/cache-5d732b1c86657ea0.arrow\n",
      "Loading cached processed dataset at /home/yp/.cache/huggingface/datasets/xsum/default/1.2.0/4957825a982999fbf80bca0b342793b01b2611e021ef589fb7c6250b3577b499/cache-38966a651c72a103.arrow\n",
      "Loading cached processed dataset at /home/yp/.cache/huggingface/datasets/xsum/default/1.2.0/4957825a982999fbf80bca0b342793b01b2611e021ef589fb7c6250b3577b499/cache-d19c279816291ade.arrow\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "ext_path = os.path.abspath('./ext_attns/')\n",
    "import sys\n",
    "sys.path.append(ext_path)\n",
    "# list\n",
    "from model.attention.UFOAttention import *\n",
    "from model.attention.AFT import AFT_FULL\n",
    "from model.attention.MUSEAttention import MUSEAttention\n",
    "from model.attention.EMSA import EMSA\n",
    "from model.attention.SimplifiedSelfAttention import SimplifiedScaledDotProductAttention\n",
    "from model.attention.SelfAttention import ScaledDotProductAttention\n",
    "from model.attention.ExternalAttention import ExternalAttention\n",
    "d_model = 768 \n",
    "n_head  = 12  \n",
    "ufo = UFOAttention(d_model=d_model, d_k=d_model//n_head, d_v=d_model//n_head, h=n_head)\n",
    "aft_full = AFT_FULL(d_model=d_model, n=n_head)\n",
    "ma = MUSEAttention(d_model=d_model, d_k=d_model//n_head, d_v=d_model//n_head, h=n_head)\n",
    "emsa = EMSA(d_model=d_model, d_k=d_model//n_head, d_v=d_model//n_head, h=n_head,H=n_head,W=n_head,ratio=2,apply_transform=True)\n",
    "ssa = SimplifiedScaledDotProductAttention(d_model=d_model, h=n_head)\n",
    "sa = ScaledDotProductAttention(d_model=d_model, d_k=d_model//n_head, d_v=d_model//n_head, h=n_head)\n",
    "ea = ExternalAttention(d_model=d_model,S=8) #v\n",
    "\n",
    "attn_li = [ufo,aft_full,ma,emsa,ssa,sa,ea]\n",
    "\n",
    "import transformers\n",
    "\n",
    "# print(transformers.__version__)\n",
    "\n",
    "model_checkpoint = \"t5-small\"\n",
    "model_checkpoint = 'facebook/bart-base'\n",
    "\n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "raw_datasets = load_dataset(\"xsum\")\n",
    "metric = load_metric(\"rouge\")\n",
    "\n",
    "# raw_datasets[\"train\"][0]\n",
    "\n",
    "import datasets\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# ?\n",
    "# with tokenizer.as_target_tokenizer():\n",
    "#     print(tokenizer([\"Hello, this one sentence!\", \"This is another sentence.\"]))\n",
    "\n",
    "if model_checkpoint in [\"t5-small\", \"t5-base\", \"t5-larg\", \"t5-3b\", \"t5-11b\"]:\n",
    "    prefix = \"summarize: \"\n",
    "else:\n",
    "    prefix = \"\"\n",
    "\n",
    "max_input_length = 1024\n",
    "max_target_length = 128\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"document\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"summary\"], max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "batch_size = 3 #16\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Rouge expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    \n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    # Extract a few results\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    \n",
    "    # Add mean generated length\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    \n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69f2fe42-e6c3-4d3a-98ac-ba6cd363e9cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /home/yp/.cache/huggingface/datasets/xsum/default/1.2.0/4957825a982999fbf80bca0b342793b01b2611e021ef589fb7c6250b3577b499/cache-a37fd9f11dd44add.arrow and /home/yp/.cache/huggingface/datasets/xsum/default/1.2.0/4957825a982999fbf80bca0b342793b01b2611e021ef589fb7c6250b3577b499/cache-195107633ef23937.arrow\n"
     ]
    }
   ],
   "source": [
    "trvaltest = tokenized_datasets['train'].train_test_split(test_size=0.004,train_size=0.005) # 800,1080\n",
    "# trvaltest = tokenized_datasets['train'].train_test_split(test_size=0.004,train_size=0.01) # 800,2080\n",
    "# trvaltest = tokenized_datasets['train'].train_test_split(test_size=0.004,train_size=0.02) # 800,4080\n",
    "tr = trvaltest['train']\n",
    "valtest = trvaltest['test'].train_test_split(test_size=0.5,train_size=0.5) # 208, 208\n",
    "# valtest = trvaltest['test'].train_test_split(test_size=0.5,train_size=0.5) # 408, 408\n",
    "# valtest = trvaltest['test'].train_test_split(test_size=0.05,train_size=0.05)\n",
    "val = valtest['train']\n",
    "te = valtest['test']\n",
    "\n",
    "del trvaltest\n",
    "del valtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b1d1b7-80ce-4576-a982-9130d9e2bea7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UFOAttention(\n",
      "  (fc_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (fc_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (fc_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (fc_o): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp fp16 backend\n",
      "The following columns in the training set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running training *****\n",
      "  Num examples = 1020\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 3\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 6\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2550\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='681' max='2550' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 681/2550 08:00 < 22:02, 1.41 it/s, Epoch 4/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>7.253441</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.911800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>6.886457</td>\n",
       "      <td>2.917300</td>\n",
       "      <td>0.219800</td>\n",
       "      <td>2.664200</td>\n",
       "      <td>2.669900</td>\n",
       "      <td>9.453400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>7.255100</td>\n",
       "      <td>6.739688</td>\n",
       "      <td>4.875600</td>\n",
       "      <td>0.333200</td>\n",
       "      <td>4.294900</td>\n",
       "      <td>4.270600</td>\n",
       "      <td>10.210800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='68' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/68 00:04 < 00:32, 1.80 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/UFOAttention/checkpoint-170\n",
      "Configuration saved in ./output/UFOAttention/checkpoint-170/config.json\n",
      "Model weights saved in ./output/UFOAttention/checkpoint-170/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/UFOAttention/checkpoint-170/tokenizer_config.json\n",
      "Special tokens file saved in ./output/UFOAttention/checkpoint-170/special_tokens_map.json\n",
      "Deleting older checkpoint [output/UFOAttention/checkpoint-7] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/UFOAttention/checkpoint-340\n",
      "Configuration saved in ./output/UFOAttention/checkpoint-340/config.json\n",
      "Model weights saved in ./output/UFOAttention/checkpoint-340/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/UFOAttention/checkpoint-340/tokenizer_config.json\n",
      "Special tokens file saved in ./output/UFOAttention/checkpoint-340/special_tokens_map.json\n",
      "Deleting older checkpoint [output/UFOAttention/checkpoint-14] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/UFOAttention/checkpoint-510\n",
      "Configuration saved in ./output/UFOAttention/checkpoint-510/config.json\n",
      "Model weights saved in ./output/UFOAttention/checkpoint-510/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/UFOAttention/checkpoint-510/tokenizer_config.json\n",
      "Special tokens file saved in ./output/UFOAttention/checkpoint-510/special_tokens_map.json\n",
      "Deleting older checkpoint [output/UFOAttention/checkpoint-21] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "evals = {}\n",
    "for i,attn in enumerate(attn_li[:]):\n",
    "    not_allowed = ('AFT_FULL','EMSA')\n",
    "    print(attn)\n",
    "    if attn.__str__().startswith(not_allowed):\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    import os\n",
    "    model_name = attn.__str__().split('(')[0]\n",
    "    # model_name = model_checkpoint.split(\"/\")[-1] + model_name\n",
    "    saved_path = os.path.join('./output',model_name)\n",
    "    \n",
    "    args = Seq2SeqTrainingArguments(\n",
    "        saved_path,\n",
    "        evaluation_strategy = \"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        weight_decay=0.01,\n",
    "        save_total_limit=3,\n",
    "        num_train_epochs=15,\n",
    "        predict_with_generate=True,\n",
    "        fp16=True,\n",
    "        save_strategy='epoch',\n",
    "        load_best_model_at_end=True,\n",
    "    #     push_to_hub=True,\n",
    "    )\n",
    "\n",
    "    class tel2lin1(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(tel2lin1,self).__init__()\n",
    "            # self.tel = nn.TransformerEncoderLayer(d_model=768,nhead=3,batch_first=True)\n",
    "            self.tel = attn\n",
    "            self.lin = nn.Linear(in_features=768,out_features=50265, bias=False)\n",
    "            \n",
    "        def forward(self,x):\n",
    "            if self.tel.__str__().startswith('ExternalAttention'):\n",
    "                x = self.tel(x)\n",
    "            else:\n",
    "                x = self.tel(x,x,x)\n",
    "            x = self.lin(x)\n",
    "            return x\n",
    "    model.lm_head = tel2lin1()\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model,\n",
    "        args,\n",
    "        train_dataset=tr,\n",
    "        eval_dataset=val,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    \n",
    "    evals[f'{model_name}'] = trainer.evaluate(eval_dataset=te)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
