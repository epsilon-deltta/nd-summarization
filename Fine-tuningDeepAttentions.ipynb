{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3106297-f71d-4de9-ae2b-bbd4f16c74f3",
   "metadata": {},
   "source": [
    "### Installation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29b61dd-e00d-4668-a632-e31a53541ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install datasets transformers rouge-score nltk\n",
    "!git clone https://github.com/xmu-xiaoma666/External-Attention-pytorch.git ext_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39745406-4976-4762-b166-2fe29fe8d92e",
   "metadata": {},
   "source": [
    "### About dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53ea756c-8f5c-4258-b80a-47c2109c7c9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['attention_mask', 'document', 'id', 'input_ids', 'labels', 'summary'],\n",
       "        num_rows: 204045\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['attention_mask', 'document', 'id', 'input_ids', 'labels', 'summary'],\n",
       "        num_rows: 11332\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['attention_mask', 'document', 'id', 'input_ids', 'labels', 'summary'],\n",
       "        num_rows: 11334\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "828b258b-c091-4f2f-9f91-fda068d293f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 204045\n",
      "val:   11332\n",
      "test:  11334\n"
     ]
    }
   ],
   "source": [
    "print(f'train: {len(tokenized_datasets[\"train\"])}')\n",
    "print(f'val:   {len(tokenized_datasets[\"validation\"])}')\n",
    "print(f'test:  {len(tokenized_datasets[\"test\"])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce481b0-db3d-44c2-a62c-6927c523a9f5",
   "metadata": {},
   "source": [
    "### 2L Attentions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596aa51d-6b0b-4c8d-8316-8069c6329766",
   "metadata": {},
   "source": [
    "|i|attention | used|\n",
    "|---|--------| ----|\n",
    "|0  |UFOAttention| ✔|\n",
    "|1  | AFT_FULL   |x|\n",
    "|2  | MUSEAttention|✔ |\n",
    "|3  | EMSA|     x|\n",
    "|4  |SimplifiedScaledDotProductAttention|✔ |\n",
    "|5  |ScaledDotProductAttention|✔ |\n",
    "|6  |ExternalAttention|✔ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31a02343-6b97-41c9-89db-01f41a0151c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset xsum (/home/yp/.cache/huggingface/datasets/xsum/default/1.2.0/4957825a982999fbf80bca0b342793b01b2611e021ef589fb7c6250b3577b499)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "013a0071960543b2960aeb1f45520452",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/yp/.cache/huggingface/datasets/xsum/default/1.2.0/4957825a982999fbf80bca0b342793b01b2611e021ef589fb7c6250b3577b499/cache-5d732b1c86657ea0.arrow\n",
      "Loading cached processed dataset at /home/yp/.cache/huggingface/datasets/xsum/default/1.2.0/4957825a982999fbf80bca0b342793b01b2611e021ef589fb7c6250b3577b499/cache-38966a651c72a103.arrow\n",
      "Loading cached processed dataset at /home/yp/.cache/huggingface/datasets/xsum/default/1.2.0/4957825a982999fbf80bca0b342793b01b2611e021ef589fb7c6250b3577b499/cache-d19c279816291ade.arrow\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "ext_path = os.path.abspath('./ext_attns/')\n",
    "import sys\n",
    "sys.path.append(ext_path)\n",
    "# list\n",
    "from model.attention.UFOAttention import *\n",
    "from model.attention.AFT import AFT_FULL\n",
    "from model.attention.MUSEAttention import MUSEAttention\n",
    "from model.attention.EMSA import EMSA\n",
    "from model.attention.SimplifiedSelfAttention import SimplifiedScaledDotProductAttention\n",
    "from model.attention.SelfAttention import ScaledDotProductAttention\n",
    "from model.attention.ExternalAttention import ExternalAttention\n",
    "d_model = 768 \n",
    "n_head  = 12  \n",
    "ufo = UFOAttention(d_model=d_model, d_k=d_model//n_head, d_v=d_model//n_head, h=n_head)\n",
    "aft_full = AFT_FULL(d_model=d_model, n=n_head)\n",
    "ma = MUSEAttention(d_model=d_model, d_k=d_model//n_head, d_v=d_model//n_head, h=n_head)\n",
    "emsa = EMSA(d_model=d_model, d_k=d_model//n_head, d_v=d_model//n_head, h=n_head,H=n_head,W=n_head,ratio=2,apply_transform=True)\n",
    "ssa = SimplifiedScaledDotProductAttention(d_model=d_model, h=n_head)\n",
    "sa = ScaledDotProductAttention(d_model=d_model, d_k=d_model//n_head, d_v=d_model//n_head, h=n_head)\n",
    "ea = ExternalAttention(d_model=d_model,S=8) #v\n",
    "\n",
    "attn_li = [ufo,aft_full,ma,emsa,ssa,sa,ea]\n",
    "\n",
    "import transformers\n",
    "\n",
    "# print(transformers.__version__)\n",
    "\n",
    "model_checkpoint = \"t5-small\"\n",
    "model_checkpoint = 'facebook/bart-base'\n",
    "\n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "raw_datasets = load_dataset(\"xsum\")\n",
    "metric = load_metric(\"rouge\")\n",
    "\n",
    "# raw_datasets[\"train\"][0]\n",
    "\n",
    "import datasets\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# ?\n",
    "# with tokenizer.as_target_tokenizer():\n",
    "#     print(tokenizer([\"Hello, this one sentence!\", \"This is another sentence.\"]))\n",
    "\n",
    "if model_checkpoint in [\"t5-small\", \"t5-base\", \"t5-larg\", \"t5-3b\", \"t5-11b\"]:\n",
    "    prefix = \"summarize: \"\n",
    "else:\n",
    "    prefix = \"\"\n",
    "\n",
    "max_input_length = 1024\n",
    "max_target_length = 128\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"document\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"summary\"], max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "batch_size = 3 #16\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Rouge expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    \n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    # Extract a few results\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    \n",
    "    # Add mean generated length\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    \n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69f2fe42-e6c3-4d3a-98ac-ba6cd363e9cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /home/yp/.cache/huggingface/datasets/xsum/default/1.2.0/4957825a982999fbf80bca0b342793b01b2611e021ef589fb7c6250b3577b499/cache-a37fd9f11dd44add.arrow and /home/yp/.cache/huggingface/datasets/xsum/default/1.2.0/4957825a982999fbf80bca0b342793b01b2611e021ef589fb7c6250b3577b499/cache-195107633ef23937.arrow\n"
     ]
    }
   ],
   "source": [
    "trvaltest = tokenized_datasets['train'].train_test_split(test_size=0.004,train_size=0.005) # 800,1080\n",
    "# trvaltest = tokenized_datasets['train'].train_test_split(test_size=0.004,train_size=0.01) # 800,2080\n",
    "# trvaltest = tokenized_datasets['train'].train_test_split(test_size=0.004,train_size=0.02) # 800,4080\n",
    "tr = trvaltest['train']\n",
    "valtest = trvaltest['test'].train_test_split(test_size=0.5,train_size=0.5) # 208, 208\n",
    "# valtest = trvaltest['test'].train_test_split(test_size=0.5,train_size=0.5) # 408, 408\n",
    "# valtest = trvaltest['test'].train_test_split(test_size=0.05,train_size=0.05)\n",
    "val = valtest['train']\n",
    "te = valtest['test']\n",
    "\n",
    "del trvaltest\n",
    "del valtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51b1d1b7-80ce-4576-a982-9130d9e2bea7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UFOAttention(\n",
      "  (fc_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (fc_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (fc_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (fc_o): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp fp16 backend\n",
      "The following columns in the training set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running training *****\n",
      "  Num examples = 1020\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 3\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 6\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2550\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2550' max='2550' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2550/2550 33:08, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>7.253441</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.911800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>6.886457</td>\n",
       "      <td>2.917300</td>\n",
       "      <td>0.219800</td>\n",
       "      <td>2.664200</td>\n",
       "      <td>2.669900</td>\n",
       "      <td>9.453400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>7.255100</td>\n",
       "      <td>6.739688</td>\n",
       "      <td>4.875600</td>\n",
       "      <td>0.333200</td>\n",
       "      <td>4.294900</td>\n",
       "      <td>4.270600</td>\n",
       "      <td>10.210800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7.255100</td>\n",
       "      <td>6.627885</td>\n",
       "      <td>10.141200</td>\n",
       "      <td>0.946400</td>\n",
       "      <td>9.057300</td>\n",
       "      <td>9.035600</td>\n",
       "      <td>19.144600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>7.255100</td>\n",
       "      <td>6.531991</td>\n",
       "      <td>9.306900</td>\n",
       "      <td>1.111700</td>\n",
       "      <td>8.489900</td>\n",
       "      <td>8.506700</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>5.949700</td>\n",
       "      <td>6.498371</td>\n",
       "      <td>11.935600</td>\n",
       "      <td>1.636700</td>\n",
       "      <td>10.323700</td>\n",
       "      <td>10.276100</td>\n",
       "      <td>19.698500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>5.949700</td>\n",
       "      <td>6.465492</td>\n",
       "      <td>11.428300</td>\n",
       "      <td>1.423800</td>\n",
       "      <td>9.882300</td>\n",
       "      <td>9.887100</td>\n",
       "      <td>16.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>5.949700</td>\n",
       "      <td>6.432230</td>\n",
       "      <td>13.255900</td>\n",
       "      <td>1.666600</td>\n",
       "      <td>11.264200</td>\n",
       "      <td>11.252400</td>\n",
       "      <td>17.576000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>5.559900</td>\n",
       "      <td>6.386285</td>\n",
       "      <td>11.311000</td>\n",
       "      <td>1.510300</td>\n",
       "      <td>9.847400</td>\n",
       "      <td>9.857600</td>\n",
       "      <td>14.951000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>5.559900</td>\n",
       "      <td>6.381336</td>\n",
       "      <td>10.845100</td>\n",
       "      <td>1.428200</td>\n",
       "      <td>9.429900</td>\n",
       "      <td>9.442700</td>\n",
       "      <td>14.757400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>5.559900</td>\n",
       "      <td>6.374988</td>\n",
       "      <td>12.600800</td>\n",
       "      <td>1.577100</td>\n",
       "      <td>10.814000</td>\n",
       "      <td>10.847700</td>\n",
       "      <td>17.120100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>5.345000</td>\n",
       "      <td>6.352579</td>\n",
       "      <td>11.608900</td>\n",
       "      <td>1.410700</td>\n",
       "      <td>10.129800</td>\n",
       "      <td>10.104700</td>\n",
       "      <td>15.941200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>5.345000</td>\n",
       "      <td>6.369030</td>\n",
       "      <td>12.296000</td>\n",
       "      <td>1.445200</td>\n",
       "      <td>10.567700</td>\n",
       "      <td>10.603400</td>\n",
       "      <td>16.789200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>5.345000</td>\n",
       "      <td>6.383066</td>\n",
       "      <td>12.121500</td>\n",
       "      <td>1.471700</td>\n",
       "      <td>10.618800</td>\n",
       "      <td>10.618400</td>\n",
       "      <td>16.308800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>5.199900</td>\n",
       "      <td>6.371593</td>\n",
       "      <td>12.078000</td>\n",
       "      <td>1.494300</td>\n",
       "      <td>10.490900</td>\n",
       "      <td>10.474800</td>\n",
       "      <td>16.242600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/UFOAttention/checkpoint-170\n",
      "Configuration saved in ./output/UFOAttention/checkpoint-170/config.json\n",
      "Model weights saved in ./output/UFOAttention/checkpoint-170/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/UFOAttention/checkpoint-170/tokenizer_config.json\n",
      "Special tokens file saved in ./output/UFOAttention/checkpoint-170/special_tokens_map.json\n",
      "Deleting older checkpoint [output/UFOAttention/checkpoint-7] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/UFOAttention/checkpoint-340\n",
      "Configuration saved in ./output/UFOAttention/checkpoint-340/config.json\n",
      "Model weights saved in ./output/UFOAttention/checkpoint-340/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/UFOAttention/checkpoint-340/tokenizer_config.json\n",
      "Special tokens file saved in ./output/UFOAttention/checkpoint-340/special_tokens_map.json\n",
      "Deleting older checkpoint [output/UFOAttention/checkpoint-14] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/UFOAttention/checkpoint-510\n",
      "Configuration saved in ./output/UFOAttention/checkpoint-510/config.json\n",
      "Model weights saved in ./output/UFOAttention/checkpoint-510/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/UFOAttention/checkpoint-510/tokenizer_config.json\n",
      "Special tokens file saved in ./output/UFOAttention/checkpoint-510/special_tokens_map.json\n",
      "Deleting older checkpoint [output/UFOAttention/checkpoint-21] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/UFOAttention/checkpoint-680\n",
      "Configuration saved in ./output/UFOAttention/checkpoint-680/config.json\n",
      "Model weights saved in ./output/UFOAttention/checkpoint-680/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/UFOAttention/checkpoint-680/tokenizer_config.json\n",
      "Special tokens file saved in ./output/UFOAttention/checkpoint-680/special_tokens_map.json\n",
      "Deleting older checkpoint [output/UFOAttention/checkpoint-170] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/UFOAttention/checkpoint-850\n",
      "Configuration saved in ./output/UFOAttention/checkpoint-850/config.json\n",
      "Model weights saved in ./output/UFOAttention/checkpoint-850/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/UFOAttention/checkpoint-850/tokenizer_config.json\n",
      "Special tokens file saved in ./output/UFOAttention/checkpoint-850/special_tokens_map.json\n",
      "Deleting older checkpoint [output/UFOAttention/checkpoint-340] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/UFOAttention/checkpoint-1020\n",
      "Configuration saved in ./output/UFOAttention/checkpoint-1020/config.json\n",
      "Model weights saved in ./output/UFOAttention/checkpoint-1020/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/UFOAttention/checkpoint-1020/tokenizer_config.json\n",
      "Special tokens file saved in ./output/UFOAttention/checkpoint-1020/special_tokens_map.json\n",
      "Deleting older checkpoint [output/UFOAttention/checkpoint-510] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/UFOAttention/checkpoint-1190\n",
      "Configuration saved in ./output/UFOAttention/checkpoint-1190/config.json\n",
      "Model weights saved in ./output/UFOAttention/checkpoint-1190/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/UFOAttention/checkpoint-1190/tokenizer_config.json\n",
      "Special tokens file saved in ./output/UFOAttention/checkpoint-1190/special_tokens_map.json\n",
      "Deleting older checkpoint [output/UFOAttention/checkpoint-680] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/UFOAttention/checkpoint-1360\n",
      "Configuration saved in ./output/UFOAttention/checkpoint-1360/config.json\n",
      "Model weights saved in ./output/UFOAttention/checkpoint-1360/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/UFOAttention/checkpoint-1360/tokenizer_config.json\n",
      "Special tokens file saved in ./output/UFOAttention/checkpoint-1360/special_tokens_map.json\n",
      "Deleting older checkpoint [output/UFOAttention/checkpoint-850] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/UFOAttention/checkpoint-1530\n",
      "Configuration saved in ./output/UFOAttention/checkpoint-1530/config.json\n",
      "Model weights saved in ./output/UFOAttention/checkpoint-1530/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/UFOAttention/checkpoint-1530/tokenizer_config.json\n",
      "Special tokens file saved in ./output/UFOAttention/checkpoint-1530/special_tokens_map.json\n",
      "Deleting older checkpoint [output/UFOAttention/checkpoint-1020] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/UFOAttention/checkpoint-1700\n",
      "Configuration saved in ./output/UFOAttention/checkpoint-1700/config.json\n",
      "Model weights saved in ./output/UFOAttention/checkpoint-1700/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/UFOAttention/checkpoint-1700/tokenizer_config.json\n",
      "Special tokens file saved in ./output/UFOAttention/checkpoint-1700/special_tokens_map.json\n",
      "Deleting older checkpoint [output/UFOAttention/checkpoint-1190] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/UFOAttention/checkpoint-1870\n",
      "Configuration saved in ./output/UFOAttention/checkpoint-1870/config.json\n",
      "Model weights saved in ./output/UFOAttention/checkpoint-1870/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/UFOAttention/checkpoint-1870/tokenizer_config.json\n",
      "Special tokens file saved in ./output/UFOAttention/checkpoint-1870/special_tokens_map.json\n",
      "Deleting older checkpoint [output/UFOAttention/checkpoint-1360] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/UFOAttention/checkpoint-2040\n",
      "Configuration saved in ./output/UFOAttention/checkpoint-2040/config.json\n",
      "Model weights saved in ./output/UFOAttention/checkpoint-2040/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/UFOAttention/checkpoint-2040/tokenizer_config.json\n",
      "Special tokens file saved in ./output/UFOAttention/checkpoint-2040/special_tokens_map.json\n",
      "Deleting older checkpoint [output/UFOAttention/checkpoint-1530] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/UFOAttention/checkpoint-2210\n",
      "Configuration saved in ./output/UFOAttention/checkpoint-2210/config.json\n",
      "Model weights saved in ./output/UFOAttention/checkpoint-2210/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/UFOAttention/checkpoint-2210/tokenizer_config.json\n",
      "Special tokens file saved in ./output/UFOAttention/checkpoint-2210/special_tokens_map.json\n",
      "Deleting older checkpoint [output/UFOAttention/checkpoint-1700] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/UFOAttention/checkpoint-2380\n",
      "Configuration saved in ./output/UFOAttention/checkpoint-2380/config.json\n",
      "Model weights saved in ./output/UFOAttention/checkpoint-2380/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/UFOAttention/checkpoint-2380/tokenizer_config.json\n",
      "Special tokens file saved in ./output/UFOAttention/checkpoint-2380/special_tokens_map.json\n",
      "Deleting older checkpoint [output/UFOAttention/checkpoint-1870] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/UFOAttention/checkpoint-2550\n",
      "Configuration saved in ./output/UFOAttention/checkpoint-2550/config.json\n",
      "Model weights saved in ./output/UFOAttention/checkpoint-2550/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/UFOAttention/checkpoint-2550/tokenizer_config.json\n",
      "Special tokens file saved in ./output/UFOAttention/checkpoint-2550/special_tokens_map.json\n",
      "Deleting older checkpoint [output/UFOAttention/checkpoint-2210] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./output/UFOAttention/checkpoint-2040 (score: 6.352578639984131).\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 409\n",
      "  Batch size = 6\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='69' max='69' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [69/69 00:38]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AFT_FULL(\n",
      "  (fc_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (fc_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (fc_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "MUSEAttention(\n",
      "  (fc_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (fc_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (fc_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (fc_o): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (conv1): Depth_Pointwise_Conv1d(\n",
      "    (depth_conv): Identity()\n",
      "    (pointwise_conv): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
      "  )\n",
      "  (conv3): Depth_Pointwise_Conv1d(\n",
      "    (depth_conv): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
      "    (pointwise_conv): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
      "  )\n",
      "  (conv5): Depth_Pointwise_Conv1d(\n",
      "    (depth_conv): Conv1d(768, 768, kernel_size=(5,), stride=(1,), padding=(2,), groups=768)\n",
      "    (pointwise_conv): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
      "  )\n",
      "  (softmax): Softmax(dim=-1)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp fp16 backend\n",
      "The following columns in the training set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running training *****\n",
      "  Num examples = 1020\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 3\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 6\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2550\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2550' max='2550' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2550/2550 33:43, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>7.041088</td>\n",
       "      <td>3.620000</td>\n",
       "      <td>0.267000</td>\n",
       "      <td>3.432400</td>\n",
       "      <td>3.419400</td>\n",
       "      <td>9.676500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>6.644437</td>\n",
       "      <td>12.680300</td>\n",
       "      <td>0.504300</td>\n",
       "      <td>10.592700</td>\n",
       "      <td>10.551800</td>\n",
       "      <td>19.948500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6.940900</td>\n",
       "      <td>6.484882</td>\n",
       "      <td>10.491200</td>\n",
       "      <td>0.260700</td>\n",
       "      <td>9.310400</td>\n",
       "      <td>9.307700</td>\n",
       "      <td>19.919100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>6.940900</td>\n",
       "      <td>6.404532</td>\n",
       "      <td>8.380500</td>\n",
       "      <td>0.060700</td>\n",
       "      <td>7.495900</td>\n",
       "      <td>7.494200</td>\n",
       "      <td>19.701000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>6.940900</td>\n",
       "      <td>6.314076</td>\n",
       "      <td>8.036400</td>\n",
       "      <td>0.091800</td>\n",
       "      <td>7.139500</td>\n",
       "      <td>7.124000</td>\n",
       "      <td>19.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>5.450200</td>\n",
       "      <td>6.303671</td>\n",
       "      <td>7.648600</td>\n",
       "      <td>0.042200</td>\n",
       "      <td>7.153300</td>\n",
       "      <td>7.133300</td>\n",
       "      <td>19.794100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>5.450200</td>\n",
       "      <td>6.306914</td>\n",
       "      <td>6.899900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.423700</td>\n",
       "      <td>6.400700</td>\n",
       "      <td>19.245100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>5.450200</td>\n",
       "      <td>6.286471</td>\n",
       "      <td>7.036200</td>\n",
       "      <td>0.013600</td>\n",
       "      <td>6.339200</td>\n",
       "      <td>6.331000</td>\n",
       "      <td>17.924000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>4.980900</td>\n",
       "      <td>6.262648</td>\n",
       "      <td>7.051800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.435800</td>\n",
       "      <td>6.410900</td>\n",
       "      <td>15.960800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.980900</td>\n",
       "      <td>6.267742</td>\n",
       "      <td>6.930200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.370400</td>\n",
       "      <td>6.352200</td>\n",
       "      <td>15.713200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>4.980900</td>\n",
       "      <td>6.302948</td>\n",
       "      <td>7.023200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.355800</td>\n",
       "      <td>6.343700</td>\n",
       "      <td>15.134800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>4.683800</td>\n",
       "      <td>6.312423</td>\n",
       "      <td>6.853600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.262600</td>\n",
       "      <td>6.249900</td>\n",
       "      <td>14.166700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>4.683800</td>\n",
       "      <td>6.328029</td>\n",
       "      <td>6.956500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.369600</td>\n",
       "      <td>6.355000</td>\n",
       "      <td>15.647100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>4.683800</td>\n",
       "      <td>6.345231</td>\n",
       "      <td>6.414100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.935000</td>\n",
       "      <td>5.929500</td>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>4.484100</td>\n",
       "      <td>6.349696</td>\n",
       "      <td>6.602300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.086900</td>\n",
       "      <td>6.069200</td>\n",
       "      <td>15.098000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/MUSEAttention/checkpoint-170\n",
      "Configuration saved in ./output/MUSEAttention/checkpoint-170/config.json\n",
      "Model weights saved in ./output/MUSEAttention/checkpoint-170/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/MUSEAttention/checkpoint-170/tokenizer_config.json\n",
      "Special tokens file saved in ./output/MUSEAttention/checkpoint-170/special_tokens_map.json\n",
      "Deleting older checkpoint [output/MUSEAttention/checkpoint-7] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/MUSEAttention/checkpoint-340\n",
      "Configuration saved in ./output/MUSEAttention/checkpoint-340/config.json\n",
      "Model weights saved in ./output/MUSEAttention/checkpoint-340/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/MUSEAttention/checkpoint-340/tokenizer_config.json\n",
      "Special tokens file saved in ./output/MUSEAttention/checkpoint-340/special_tokens_map.json\n",
      "Deleting older checkpoint [output/MUSEAttention/checkpoint-14] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/MUSEAttention/checkpoint-510\n",
      "Configuration saved in ./output/MUSEAttention/checkpoint-510/config.json\n",
      "Model weights saved in ./output/MUSEAttention/checkpoint-510/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/MUSEAttention/checkpoint-510/tokenizer_config.json\n",
      "Special tokens file saved in ./output/MUSEAttention/checkpoint-510/special_tokens_map.json\n",
      "Deleting older checkpoint [output/MUSEAttention/checkpoint-21] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/MUSEAttention/checkpoint-680\n",
      "Configuration saved in ./output/MUSEAttention/checkpoint-680/config.json\n",
      "Model weights saved in ./output/MUSEAttention/checkpoint-680/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/MUSEAttention/checkpoint-680/tokenizer_config.json\n",
      "Special tokens file saved in ./output/MUSEAttention/checkpoint-680/special_tokens_map.json\n",
      "Deleting older checkpoint [output/MUSEAttention/checkpoint-170] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/MUSEAttention/checkpoint-850\n",
      "Configuration saved in ./output/MUSEAttention/checkpoint-850/config.json\n",
      "Model weights saved in ./output/MUSEAttention/checkpoint-850/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/MUSEAttention/checkpoint-850/tokenizer_config.json\n",
      "Special tokens file saved in ./output/MUSEAttention/checkpoint-850/special_tokens_map.json\n",
      "Deleting older checkpoint [output/MUSEAttention/checkpoint-340] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/MUSEAttention/checkpoint-1020\n",
      "Configuration saved in ./output/MUSEAttention/checkpoint-1020/config.json\n",
      "Model weights saved in ./output/MUSEAttention/checkpoint-1020/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/MUSEAttention/checkpoint-1020/tokenizer_config.json\n",
      "Special tokens file saved in ./output/MUSEAttention/checkpoint-1020/special_tokens_map.json\n",
      "Deleting older checkpoint [output/MUSEAttention/checkpoint-510] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/MUSEAttention/checkpoint-1190\n",
      "Configuration saved in ./output/MUSEAttention/checkpoint-1190/config.json\n",
      "Model weights saved in ./output/MUSEAttention/checkpoint-1190/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/MUSEAttention/checkpoint-1190/tokenizer_config.json\n",
      "Special tokens file saved in ./output/MUSEAttention/checkpoint-1190/special_tokens_map.json\n",
      "Deleting older checkpoint [output/MUSEAttention/checkpoint-680] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/MUSEAttention/checkpoint-1360\n",
      "Configuration saved in ./output/MUSEAttention/checkpoint-1360/config.json\n",
      "Model weights saved in ./output/MUSEAttention/checkpoint-1360/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/MUSEAttention/checkpoint-1360/tokenizer_config.json\n",
      "Special tokens file saved in ./output/MUSEAttention/checkpoint-1360/special_tokens_map.json\n",
      "Deleting older checkpoint [output/MUSEAttention/checkpoint-850] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/MUSEAttention/checkpoint-1530\n",
      "Configuration saved in ./output/MUSEAttention/checkpoint-1530/config.json\n",
      "Model weights saved in ./output/MUSEAttention/checkpoint-1530/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/MUSEAttention/checkpoint-1530/tokenizer_config.json\n",
      "Special tokens file saved in ./output/MUSEAttention/checkpoint-1530/special_tokens_map.json\n",
      "Deleting older checkpoint [output/MUSEAttention/checkpoint-1020] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/MUSEAttention/checkpoint-1700\n",
      "Configuration saved in ./output/MUSEAttention/checkpoint-1700/config.json\n",
      "Model weights saved in ./output/MUSEAttention/checkpoint-1700/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/MUSEAttention/checkpoint-1700/tokenizer_config.json\n",
      "Special tokens file saved in ./output/MUSEAttention/checkpoint-1700/special_tokens_map.json\n",
      "Deleting older checkpoint [output/MUSEAttention/checkpoint-1190] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/MUSEAttention/checkpoint-1870\n",
      "Configuration saved in ./output/MUSEAttention/checkpoint-1870/config.json\n",
      "Model weights saved in ./output/MUSEAttention/checkpoint-1870/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/MUSEAttention/checkpoint-1870/tokenizer_config.json\n",
      "Special tokens file saved in ./output/MUSEAttention/checkpoint-1870/special_tokens_map.json\n",
      "Deleting older checkpoint [output/MUSEAttention/checkpoint-1360] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/MUSEAttention/checkpoint-2040\n",
      "Configuration saved in ./output/MUSEAttention/checkpoint-2040/config.json\n",
      "Model weights saved in ./output/MUSEAttention/checkpoint-2040/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/MUSEAttention/checkpoint-2040/tokenizer_config.json\n",
      "Special tokens file saved in ./output/MUSEAttention/checkpoint-2040/special_tokens_map.json\n",
      "Deleting older checkpoint [output/MUSEAttention/checkpoint-1700] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/MUSEAttention/checkpoint-2210\n",
      "Configuration saved in ./output/MUSEAttention/checkpoint-2210/config.json\n",
      "Model weights saved in ./output/MUSEAttention/checkpoint-2210/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/MUSEAttention/checkpoint-2210/tokenizer_config.json\n",
      "Special tokens file saved in ./output/MUSEAttention/checkpoint-2210/special_tokens_map.json\n",
      "Deleting older checkpoint [output/MUSEAttention/checkpoint-1870] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/MUSEAttention/checkpoint-2380\n",
      "Configuration saved in ./output/MUSEAttention/checkpoint-2380/config.json\n",
      "Model weights saved in ./output/MUSEAttention/checkpoint-2380/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/MUSEAttention/checkpoint-2380/tokenizer_config.json\n",
      "Special tokens file saved in ./output/MUSEAttention/checkpoint-2380/special_tokens_map.json\n",
      "Deleting older checkpoint [output/MUSEAttention/checkpoint-2040] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/MUSEAttention/checkpoint-2550\n",
      "Configuration saved in ./output/MUSEAttention/checkpoint-2550/config.json\n",
      "Model weights saved in ./output/MUSEAttention/checkpoint-2550/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/MUSEAttention/checkpoint-2550/tokenizer_config.json\n",
      "Special tokens file saved in ./output/MUSEAttention/checkpoint-2550/special_tokens_map.json\n",
      "Deleting older checkpoint [output/MUSEAttention/checkpoint-2210] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./output/MUSEAttention/checkpoint-1530 (score: 6.26264762878418).\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 409\n",
      "  Batch size = 6\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='69' max='69' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [69/69 00:39]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMSA(\n",
      "  (fc_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (fc_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (fc_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (fc_o): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (sr): Sequential()\n",
      "  (sr_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=768)\n",
      "  (sr_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (transform): Sequential(\n",
      "    (conv): Conv2d(12, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (softmax): Softmax(dim=-1)\n",
      "    (in): InstanceNorm2d(12, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "  )\n",
      ")\n",
      "SimplifiedScaledDotProductAttention(\n",
      "  (fc_o): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp fp16 backend\n",
      "The following columns in the training set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running training *****\n",
      "  Num examples = 1020\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 3\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 6\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2550\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2550' max='2550' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2550/2550 33:01, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>6.857039</td>\n",
       "      <td>0.900600</td>\n",
       "      <td>0.070500</td>\n",
       "      <td>0.827600</td>\n",
       "      <td>0.830300</td>\n",
       "      <td>5.058800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>6.544621</td>\n",
       "      <td>4.981200</td>\n",
       "      <td>0.323700</td>\n",
       "      <td>4.562200</td>\n",
       "      <td>4.556600</td>\n",
       "      <td>11.850500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6.699700</td>\n",
       "      <td>6.492774</td>\n",
       "      <td>7.326900</td>\n",
       "      <td>0.859100</td>\n",
       "      <td>6.453700</td>\n",
       "      <td>6.464000</td>\n",
       "      <td>14.272100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>6.699700</td>\n",
       "      <td>6.518483</td>\n",
       "      <td>7.967800</td>\n",
       "      <td>0.928100</td>\n",
       "      <td>6.736600</td>\n",
       "      <td>6.727800</td>\n",
       "      <td>13.492600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>6.699700</td>\n",
       "      <td>6.501227</td>\n",
       "      <td>10.485200</td>\n",
       "      <td>1.274500</td>\n",
       "      <td>8.785700</td>\n",
       "      <td>8.787300</td>\n",
       "      <td>15.747500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>5.267500</td>\n",
       "      <td>6.539152</td>\n",
       "      <td>10.108000</td>\n",
       "      <td>1.346200</td>\n",
       "      <td>8.432300</td>\n",
       "      <td>8.448000</td>\n",
       "      <td>14.676500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>5.267500</td>\n",
       "      <td>6.550370</td>\n",
       "      <td>9.858800</td>\n",
       "      <td>1.310700</td>\n",
       "      <td>7.964600</td>\n",
       "      <td>7.980100</td>\n",
       "      <td>14.316200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>5.267500</td>\n",
       "      <td>6.601139</td>\n",
       "      <td>10.809700</td>\n",
       "      <td>1.307700</td>\n",
       "      <td>9.111300</td>\n",
       "      <td>9.125700</td>\n",
       "      <td>15.512300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>4.928700</td>\n",
       "      <td>6.559929</td>\n",
       "      <td>10.210900</td>\n",
       "      <td>1.326500</td>\n",
       "      <td>8.512400</td>\n",
       "      <td>8.556700</td>\n",
       "      <td>14.735300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.928700</td>\n",
       "      <td>6.573251</td>\n",
       "      <td>10.389400</td>\n",
       "      <td>1.233700</td>\n",
       "      <td>8.659500</td>\n",
       "      <td>8.666600</td>\n",
       "      <td>15.338200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>4.928700</td>\n",
       "      <td>6.606375</td>\n",
       "      <td>10.843400</td>\n",
       "      <td>1.449500</td>\n",
       "      <td>8.794100</td>\n",
       "      <td>8.815400</td>\n",
       "      <td>15.590700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>4.771900</td>\n",
       "      <td>6.603196</td>\n",
       "      <td>11.838300</td>\n",
       "      <td>1.449600</td>\n",
       "      <td>9.585800</td>\n",
       "      <td>9.575700</td>\n",
       "      <td>16.002500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>4.771900</td>\n",
       "      <td>6.622152</td>\n",
       "      <td>11.505800</td>\n",
       "      <td>1.382100</td>\n",
       "      <td>9.328600</td>\n",
       "      <td>9.332500</td>\n",
       "      <td>16.105400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>4.771900</td>\n",
       "      <td>6.623005</td>\n",
       "      <td>11.659600</td>\n",
       "      <td>1.419700</td>\n",
       "      <td>9.514800</td>\n",
       "      <td>9.530000</td>\n",
       "      <td>16.727900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>4.637900</td>\n",
       "      <td>6.625597</td>\n",
       "      <td>11.634300</td>\n",
       "      <td>1.424600</td>\n",
       "      <td>9.628400</td>\n",
       "      <td>9.624700</td>\n",
       "      <td>16.708300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/SimplifiedScaledDotProductAttention/checkpoint-170\n",
      "Configuration saved in ./output/SimplifiedScaledDotProductAttention/checkpoint-170/config.json\n",
      "Model weights saved in ./output/SimplifiedScaledDotProductAttention/checkpoint-170/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SimplifiedScaledDotProductAttention/checkpoint-170/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SimplifiedScaledDotProductAttention/checkpoint-170/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SimplifiedScaledDotProductAttention/checkpoint-7] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/SimplifiedScaledDotProductAttention/checkpoint-340\n",
      "Configuration saved in ./output/SimplifiedScaledDotProductAttention/checkpoint-340/config.json\n",
      "Model weights saved in ./output/SimplifiedScaledDotProductAttention/checkpoint-340/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SimplifiedScaledDotProductAttention/checkpoint-340/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SimplifiedScaledDotProductAttention/checkpoint-340/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SimplifiedScaledDotProductAttention/checkpoint-14] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/SimplifiedScaledDotProductAttention/checkpoint-510\n",
      "Configuration saved in ./output/SimplifiedScaledDotProductAttention/checkpoint-510/config.json\n",
      "Model weights saved in ./output/SimplifiedScaledDotProductAttention/checkpoint-510/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SimplifiedScaledDotProductAttention/checkpoint-510/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SimplifiedScaledDotProductAttention/checkpoint-510/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SimplifiedScaledDotProductAttention/checkpoint-21] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/SimplifiedScaledDotProductAttention/checkpoint-680\n",
      "Configuration saved in ./output/SimplifiedScaledDotProductAttention/checkpoint-680/config.json\n",
      "Model weights saved in ./output/SimplifiedScaledDotProductAttention/checkpoint-680/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SimplifiedScaledDotProductAttention/checkpoint-680/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SimplifiedScaledDotProductAttention/checkpoint-680/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SimplifiedScaledDotProductAttention/checkpoint-170] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/SimplifiedScaledDotProductAttention/checkpoint-850\n",
      "Configuration saved in ./output/SimplifiedScaledDotProductAttention/checkpoint-850/config.json\n",
      "Model weights saved in ./output/SimplifiedScaledDotProductAttention/checkpoint-850/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SimplifiedScaledDotProductAttention/checkpoint-850/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SimplifiedScaledDotProductAttention/checkpoint-850/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SimplifiedScaledDotProductAttention/checkpoint-340] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/SimplifiedScaledDotProductAttention/checkpoint-1020\n",
      "Configuration saved in ./output/SimplifiedScaledDotProductAttention/checkpoint-1020/config.json\n",
      "Model weights saved in ./output/SimplifiedScaledDotProductAttention/checkpoint-1020/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SimplifiedScaledDotProductAttention/checkpoint-1020/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SimplifiedScaledDotProductAttention/checkpoint-1020/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SimplifiedScaledDotProductAttention/checkpoint-680] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/SimplifiedScaledDotProductAttention/checkpoint-1190\n",
      "Configuration saved in ./output/SimplifiedScaledDotProductAttention/checkpoint-1190/config.json\n",
      "Model weights saved in ./output/SimplifiedScaledDotProductAttention/checkpoint-1190/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SimplifiedScaledDotProductAttention/checkpoint-1190/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SimplifiedScaledDotProductAttention/checkpoint-1190/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SimplifiedScaledDotProductAttention/checkpoint-850] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/SimplifiedScaledDotProductAttention/checkpoint-1360\n",
      "Configuration saved in ./output/SimplifiedScaledDotProductAttention/checkpoint-1360/config.json\n",
      "Model weights saved in ./output/SimplifiedScaledDotProductAttention/checkpoint-1360/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SimplifiedScaledDotProductAttention/checkpoint-1360/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SimplifiedScaledDotProductAttention/checkpoint-1360/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SimplifiedScaledDotProductAttention/checkpoint-1020] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/SimplifiedScaledDotProductAttention/checkpoint-1530\n",
      "Configuration saved in ./output/SimplifiedScaledDotProductAttention/checkpoint-1530/config.json\n",
      "Model weights saved in ./output/SimplifiedScaledDotProductAttention/checkpoint-1530/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SimplifiedScaledDotProductAttention/checkpoint-1530/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SimplifiedScaledDotProductAttention/checkpoint-1530/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SimplifiedScaledDotProductAttention/checkpoint-1190] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/SimplifiedScaledDotProductAttention/checkpoint-1700\n",
      "Configuration saved in ./output/SimplifiedScaledDotProductAttention/checkpoint-1700/config.json\n",
      "Model weights saved in ./output/SimplifiedScaledDotProductAttention/checkpoint-1700/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SimplifiedScaledDotProductAttention/checkpoint-1700/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SimplifiedScaledDotProductAttention/checkpoint-1700/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SimplifiedScaledDotProductAttention/checkpoint-1360] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/SimplifiedScaledDotProductAttention/checkpoint-1870\n",
      "Configuration saved in ./output/SimplifiedScaledDotProductAttention/checkpoint-1870/config.json\n",
      "Model weights saved in ./output/SimplifiedScaledDotProductAttention/checkpoint-1870/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SimplifiedScaledDotProductAttention/checkpoint-1870/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SimplifiedScaledDotProductAttention/checkpoint-1870/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SimplifiedScaledDotProductAttention/checkpoint-1530] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/SimplifiedScaledDotProductAttention/checkpoint-2040\n",
      "Configuration saved in ./output/SimplifiedScaledDotProductAttention/checkpoint-2040/config.json\n",
      "Model weights saved in ./output/SimplifiedScaledDotProductAttention/checkpoint-2040/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SimplifiedScaledDotProductAttention/checkpoint-2040/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SimplifiedScaledDotProductAttention/checkpoint-2040/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SimplifiedScaledDotProductAttention/checkpoint-1700] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/SimplifiedScaledDotProductAttention/checkpoint-2210\n",
      "Configuration saved in ./output/SimplifiedScaledDotProductAttention/checkpoint-2210/config.json\n",
      "Model weights saved in ./output/SimplifiedScaledDotProductAttention/checkpoint-2210/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SimplifiedScaledDotProductAttention/checkpoint-2210/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SimplifiedScaledDotProductAttention/checkpoint-2210/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SimplifiedScaledDotProductAttention/checkpoint-1870] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/SimplifiedScaledDotProductAttention/checkpoint-2380\n",
      "Configuration saved in ./output/SimplifiedScaledDotProductAttention/checkpoint-2380/config.json\n",
      "Model weights saved in ./output/SimplifiedScaledDotProductAttention/checkpoint-2380/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SimplifiedScaledDotProductAttention/checkpoint-2380/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SimplifiedScaledDotProductAttention/checkpoint-2380/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SimplifiedScaledDotProductAttention/checkpoint-2040] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/SimplifiedScaledDotProductAttention/checkpoint-2550\n",
      "Configuration saved in ./output/SimplifiedScaledDotProductAttention/checkpoint-2550/config.json\n",
      "Model weights saved in ./output/SimplifiedScaledDotProductAttention/checkpoint-2550/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SimplifiedScaledDotProductAttention/checkpoint-2550/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SimplifiedScaledDotProductAttention/checkpoint-2550/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SimplifiedScaledDotProductAttention/checkpoint-2210] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./output/SimplifiedScaledDotProductAttention/checkpoint-510 (score: 6.492773532867432).\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 409\n",
      "  Batch size = 6\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='69' max='69' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [69/69 00:38]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScaledDotProductAttention(\n",
      "  (fc_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (fc_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (fc_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (fc_o): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp fp16 backend\n",
      "The following columns in the training set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running training *****\n",
      "  Num examples = 1020\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 3\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 6\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2550\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2550' max='2550' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2550/2550 31:49, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>7.611351</td>\n",
       "      <td>7.668600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.643000</td>\n",
       "      <td>7.676100</td>\n",
       "      <td>7.117600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>7.116733</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.009800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>7.327000</td>\n",
       "      <td>7.085515</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7.327000</td>\n",
       "      <td>6.972314</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>7.327000</td>\n",
       "      <td>6.967639</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>5.834600</td>\n",
       "      <td>7.054784</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>5.834600</td>\n",
       "      <td>7.103719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>5.834600</td>\n",
       "      <td>7.127185</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.629900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>5.484100</td>\n",
       "      <td>7.137162</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>5.484100</td>\n",
       "      <td>7.201315</td>\n",
       "      <td>0.452200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.454600</td>\n",
       "      <td>0.459800</td>\n",
       "      <td>12.188700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>5.484100</td>\n",
       "      <td>7.313945</td>\n",
       "      <td>0.282600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.288800</td>\n",
       "      <td>0.283700</td>\n",
       "      <td>17.850500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>5.286200</td>\n",
       "      <td>7.285947</td>\n",
       "      <td>1.359600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.365200</td>\n",
       "      <td>1.352100</td>\n",
       "      <td>19.698500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>5.286200</td>\n",
       "      <td>7.349900</td>\n",
       "      <td>0.990200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.986300</td>\n",
       "      <td>0.982000</td>\n",
       "      <td>19.973000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>5.286200</td>\n",
       "      <td>7.379765</td>\n",
       "      <td>0.453700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.448000</td>\n",
       "      <td>19.951000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>5.146900</td>\n",
       "      <td>7.362760</td>\n",
       "      <td>0.557300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.563300</td>\n",
       "      <td>0.555900</td>\n",
       "      <td>19.992600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/ScaledDotProductAttention/checkpoint-170\n",
      "Configuration saved in ./output/ScaledDotProductAttention/checkpoint-170/config.json\n",
      "Model weights saved in ./output/ScaledDotProductAttention/checkpoint-170/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/ScaledDotProductAttention/checkpoint-170/tokenizer_config.json\n",
      "Special tokens file saved in ./output/ScaledDotProductAttention/checkpoint-170/special_tokens_map.json\n",
      "Deleting older checkpoint [output/ScaledDotProductAttention/checkpoint-7] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/ScaledDotProductAttention/checkpoint-340\n",
      "Configuration saved in ./output/ScaledDotProductAttention/checkpoint-340/config.json\n",
      "Model weights saved in ./output/ScaledDotProductAttention/checkpoint-340/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/ScaledDotProductAttention/checkpoint-340/tokenizer_config.json\n",
      "Special tokens file saved in ./output/ScaledDotProductAttention/checkpoint-340/special_tokens_map.json\n",
      "Deleting older checkpoint [output/ScaledDotProductAttention/checkpoint-14] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/ScaledDotProductAttention/checkpoint-510\n",
      "Configuration saved in ./output/ScaledDotProductAttention/checkpoint-510/config.json\n",
      "Model weights saved in ./output/ScaledDotProductAttention/checkpoint-510/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/ScaledDotProductAttention/checkpoint-510/tokenizer_config.json\n",
      "Special tokens file saved in ./output/ScaledDotProductAttention/checkpoint-510/special_tokens_map.json\n",
      "Deleting older checkpoint [output/ScaledDotProductAttention/checkpoint-21] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/ScaledDotProductAttention/checkpoint-680\n",
      "Configuration saved in ./output/ScaledDotProductAttention/checkpoint-680/config.json\n",
      "Model weights saved in ./output/ScaledDotProductAttention/checkpoint-680/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/ScaledDotProductAttention/checkpoint-680/tokenizer_config.json\n",
      "Special tokens file saved in ./output/ScaledDotProductAttention/checkpoint-680/special_tokens_map.json\n",
      "Deleting older checkpoint [output/ScaledDotProductAttention/checkpoint-170] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/ScaledDotProductAttention/checkpoint-850\n",
      "Configuration saved in ./output/ScaledDotProductAttention/checkpoint-850/config.json\n",
      "Model weights saved in ./output/ScaledDotProductAttention/checkpoint-850/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/ScaledDotProductAttention/checkpoint-850/tokenizer_config.json\n",
      "Special tokens file saved in ./output/ScaledDotProductAttention/checkpoint-850/special_tokens_map.json\n",
      "Deleting older checkpoint [output/ScaledDotProductAttention/checkpoint-340] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/ScaledDotProductAttention/checkpoint-1020\n",
      "Configuration saved in ./output/ScaledDotProductAttention/checkpoint-1020/config.json\n",
      "Model weights saved in ./output/ScaledDotProductAttention/checkpoint-1020/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/ScaledDotProductAttention/checkpoint-1020/tokenizer_config.json\n",
      "Special tokens file saved in ./output/ScaledDotProductAttention/checkpoint-1020/special_tokens_map.json\n",
      "Deleting older checkpoint [output/ScaledDotProductAttention/checkpoint-510] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/ScaledDotProductAttention/checkpoint-1190\n",
      "Configuration saved in ./output/ScaledDotProductAttention/checkpoint-1190/config.json\n",
      "Model weights saved in ./output/ScaledDotProductAttention/checkpoint-1190/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/ScaledDotProductAttention/checkpoint-1190/tokenizer_config.json\n",
      "Special tokens file saved in ./output/ScaledDotProductAttention/checkpoint-1190/special_tokens_map.json\n",
      "Deleting older checkpoint [output/ScaledDotProductAttention/checkpoint-680] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/ScaledDotProductAttention/checkpoint-1360\n",
      "Configuration saved in ./output/ScaledDotProductAttention/checkpoint-1360/config.json\n",
      "Model weights saved in ./output/ScaledDotProductAttention/checkpoint-1360/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/ScaledDotProductAttention/checkpoint-1360/tokenizer_config.json\n",
      "Special tokens file saved in ./output/ScaledDotProductAttention/checkpoint-1360/special_tokens_map.json\n",
      "Deleting older checkpoint [output/ScaledDotProductAttention/checkpoint-1020] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/ScaledDotProductAttention/checkpoint-1530\n",
      "Configuration saved in ./output/ScaledDotProductAttention/checkpoint-1530/config.json\n",
      "Model weights saved in ./output/ScaledDotProductAttention/checkpoint-1530/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/ScaledDotProductAttention/checkpoint-1530/tokenizer_config.json\n",
      "Special tokens file saved in ./output/ScaledDotProductAttention/checkpoint-1530/special_tokens_map.json\n",
      "Deleting older checkpoint [output/ScaledDotProductAttention/checkpoint-1190] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/ScaledDotProductAttention/checkpoint-1700\n",
      "Configuration saved in ./output/ScaledDotProductAttention/checkpoint-1700/config.json\n",
      "Model weights saved in ./output/ScaledDotProductAttention/checkpoint-1700/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/ScaledDotProductAttention/checkpoint-1700/tokenizer_config.json\n",
      "Special tokens file saved in ./output/ScaledDotProductAttention/checkpoint-1700/special_tokens_map.json\n",
      "Deleting older checkpoint [output/ScaledDotProductAttention/checkpoint-1360] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/ScaledDotProductAttention/checkpoint-1870\n",
      "Configuration saved in ./output/ScaledDotProductAttention/checkpoint-1870/config.json\n",
      "Model weights saved in ./output/ScaledDotProductAttention/checkpoint-1870/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/ScaledDotProductAttention/checkpoint-1870/tokenizer_config.json\n",
      "Special tokens file saved in ./output/ScaledDotProductAttention/checkpoint-1870/special_tokens_map.json\n",
      "Deleting older checkpoint [output/ScaledDotProductAttention/checkpoint-1530] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/ScaledDotProductAttention/checkpoint-2040\n",
      "Configuration saved in ./output/ScaledDotProductAttention/checkpoint-2040/config.json\n",
      "Model weights saved in ./output/ScaledDotProductAttention/checkpoint-2040/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/ScaledDotProductAttention/checkpoint-2040/tokenizer_config.json\n",
      "Special tokens file saved in ./output/ScaledDotProductAttention/checkpoint-2040/special_tokens_map.json\n",
      "Deleting older checkpoint [output/ScaledDotProductAttention/checkpoint-1700] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/ScaledDotProductAttention/checkpoint-2210\n",
      "Configuration saved in ./output/ScaledDotProductAttention/checkpoint-2210/config.json\n",
      "Model weights saved in ./output/ScaledDotProductAttention/checkpoint-2210/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/ScaledDotProductAttention/checkpoint-2210/tokenizer_config.json\n",
      "Special tokens file saved in ./output/ScaledDotProductAttention/checkpoint-2210/special_tokens_map.json\n",
      "Deleting older checkpoint [output/ScaledDotProductAttention/checkpoint-1870] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/ScaledDotProductAttention/checkpoint-2380\n",
      "Configuration saved in ./output/ScaledDotProductAttention/checkpoint-2380/config.json\n",
      "Model weights saved in ./output/ScaledDotProductAttention/checkpoint-2380/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/ScaledDotProductAttention/checkpoint-2380/tokenizer_config.json\n",
      "Special tokens file saved in ./output/ScaledDotProductAttention/checkpoint-2380/special_tokens_map.json\n",
      "Deleting older checkpoint [output/ScaledDotProductAttention/checkpoint-2040] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/ScaledDotProductAttention/checkpoint-2550\n",
      "Configuration saved in ./output/ScaledDotProductAttention/checkpoint-2550/config.json\n",
      "Model weights saved in ./output/ScaledDotProductAttention/checkpoint-2550/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/ScaledDotProductAttention/checkpoint-2550/tokenizer_config.json\n",
      "Special tokens file saved in ./output/ScaledDotProductAttention/checkpoint-2550/special_tokens_map.json\n",
      "Deleting older checkpoint [output/ScaledDotProductAttention/checkpoint-2210] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./output/ScaledDotProductAttention/checkpoint-850 (score: 6.967639446258545).\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 409\n",
      "  Batch size = 6\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='69' max='69' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [69/69 00:28]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExternalAttention(\n",
      "  (mk): Linear(in_features=768, out_features=8, bias=False)\n",
      "  (mv): Linear(in_features=8, out_features=768, bias=False)\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp fp16 backend\n",
      "The following columns in the training set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running training *****\n",
      "  Num examples = 1020\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 3\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 6\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2550\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2550' max='2550' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2550/2550 31:03, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>10.813046</td>\n",
       "      <td>7.761000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.724500</td>\n",
       "      <td>7.530400</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>10.768181</td>\n",
       "      <td>10.789500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.688400</td>\n",
       "      <td>10.697100</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>10.780100</td>\n",
       "      <td>10.683411</td>\n",
       "      <td>10.429300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.333300</td>\n",
       "      <td>10.432900</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>10.780100</td>\n",
       "      <td>10.577523</td>\n",
       "      <td>8.146900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.115200</td>\n",
       "      <td>7.591400</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>10.780100</td>\n",
       "      <td>10.462443</td>\n",
       "      <td>6.027400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.023500</td>\n",
       "      <td>6.038400</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>10.497300</td>\n",
       "      <td>10.346169</td>\n",
       "      <td>6.027400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.023500</td>\n",
       "      <td>6.038400</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>10.497300</td>\n",
       "      <td>10.234465</td>\n",
       "      <td>6.027400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.023500</td>\n",
       "      <td>6.038400</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>10.497300</td>\n",
       "      <td>10.132950</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>10.127700</td>\n",
       "      <td>10.041049</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>10.127700</td>\n",
       "      <td>9.963591</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>10.127700</td>\n",
       "      <td>9.899662</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>9.842600</td>\n",
       "      <td>9.849934</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>9.842600</td>\n",
       "      <td>9.814616</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>9.842600</td>\n",
       "      <td>9.793914</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>9.687300</td>\n",
       "      <td>9.787157</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/ExternalAttention/checkpoint-170\n",
      "Configuration saved in ./output/ExternalAttention/checkpoint-170/config.json\n",
      "Model weights saved in ./output/ExternalAttention/checkpoint-170/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/ExternalAttention/checkpoint-170/tokenizer_config.json\n",
      "Special tokens file saved in ./output/ExternalAttention/checkpoint-170/special_tokens_map.json\n",
      "Deleting older checkpoint [output/ExternalAttention/checkpoint-7] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/ExternalAttention/checkpoint-340\n",
      "Configuration saved in ./output/ExternalAttention/checkpoint-340/config.json\n",
      "Model weights saved in ./output/ExternalAttention/checkpoint-340/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/ExternalAttention/checkpoint-340/tokenizer_config.json\n",
      "Special tokens file saved in ./output/ExternalAttention/checkpoint-340/special_tokens_map.json\n",
      "Deleting older checkpoint [output/ExternalAttention/checkpoint-14] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/ExternalAttention/checkpoint-510\n",
      "Configuration saved in ./output/ExternalAttention/checkpoint-510/config.json\n",
      "Model weights saved in ./output/ExternalAttention/checkpoint-510/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/ExternalAttention/checkpoint-510/tokenizer_config.json\n",
      "Special tokens file saved in ./output/ExternalAttention/checkpoint-510/special_tokens_map.json\n",
      "Deleting older checkpoint [output/ExternalAttention/checkpoint-21] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/ExternalAttention/checkpoint-680\n",
      "Configuration saved in ./output/ExternalAttention/checkpoint-680/config.json\n",
      "Model weights saved in ./output/ExternalAttention/checkpoint-680/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/ExternalAttention/checkpoint-680/tokenizer_config.json\n",
      "Special tokens file saved in ./output/ExternalAttention/checkpoint-680/special_tokens_map.json\n",
      "Deleting older checkpoint [output/ExternalAttention/checkpoint-170] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/ExternalAttention/checkpoint-850\n",
      "Configuration saved in ./output/ExternalAttention/checkpoint-850/config.json\n",
      "Model weights saved in ./output/ExternalAttention/checkpoint-850/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/ExternalAttention/checkpoint-850/tokenizer_config.json\n",
      "Special tokens file saved in ./output/ExternalAttention/checkpoint-850/special_tokens_map.json\n",
      "Deleting older checkpoint [output/ExternalAttention/checkpoint-340] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/ExternalAttention/checkpoint-1020\n",
      "Configuration saved in ./output/ExternalAttention/checkpoint-1020/config.json\n",
      "Model weights saved in ./output/ExternalAttention/checkpoint-1020/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/ExternalAttention/checkpoint-1020/tokenizer_config.json\n",
      "Special tokens file saved in ./output/ExternalAttention/checkpoint-1020/special_tokens_map.json\n",
      "Deleting older checkpoint [output/ExternalAttention/checkpoint-510] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/ExternalAttention/checkpoint-1190\n",
      "Configuration saved in ./output/ExternalAttention/checkpoint-1190/config.json\n",
      "Model weights saved in ./output/ExternalAttention/checkpoint-1190/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/ExternalAttention/checkpoint-1190/tokenizer_config.json\n",
      "Special tokens file saved in ./output/ExternalAttention/checkpoint-1190/special_tokens_map.json\n",
      "Deleting older checkpoint [output/ExternalAttention/checkpoint-680] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/ExternalAttention/checkpoint-1360\n",
      "Configuration saved in ./output/ExternalAttention/checkpoint-1360/config.json\n",
      "Model weights saved in ./output/ExternalAttention/checkpoint-1360/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/ExternalAttention/checkpoint-1360/tokenizer_config.json\n",
      "Special tokens file saved in ./output/ExternalAttention/checkpoint-1360/special_tokens_map.json\n",
      "Deleting older checkpoint [output/ExternalAttention/checkpoint-850] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/ExternalAttention/checkpoint-1530\n",
      "Configuration saved in ./output/ExternalAttention/checkpoint-1530/config.json\n",
      "Model weights saved in ./output/ExternalAttention/checkpoint-1530/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/ExternalAttention/checkpoint-1530/tokenizer_config.json\n",
      "Special tokens file saved in ./output/ExternalAttention/checkpoint-1530/special_tokens_map.json\n",
      "Deleting older checkpoint [output/ExternalAttention/checkpoint-1020] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/ExternalAttention/checkpoint-1700\n",
      "Configuration saved in ./output/ExternalAttention/checkpoint-1700/config.json\n",
      "Model weights saved in ./output/ExternalAttention/checkpoint-1700/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/ExternalAttention/checkpoint-1700/tokenizer_config.json\n",
      "Special tokens file saved in ./output/ExternalAttention/checkpoint-1700/special_tokens_map.json\n",
      "Deleting older checkpoint [output/ExternalAttention/checkpoint-1190] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/ExternalAttention/checkpoint-1870\n",
      "Configuration saved in ./output/ExternalAttention/checkpoint-1870/config.json\n",
      "Model weights saved in ./output/ExternalAttention/checkpoint-1870/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/ExternalAttention/checkpoint-1870/tokenizer_config.json\n",
      "Special tokens file saved in ./output/ExternalAttention/checkpoint-1870/special_tokens_map.json\n",
      "Deleting older checkpoint [output/ExternalAttention/checkpoint-1360] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/ExternalAttention/checkpoint-2040\n",
      "Configuration saved in ./output/ExternalAttention/checkpoint-2040/config.json\n",
      "Model weights saved in ./output/ExternalAttention/checkpoint-2040/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/ExternalAttention/checkpoint-2040/tokenizer_config.json\n",
      "Special tokens file saved in ./output/ExternalAttention/checkpoint-2040/special_tokens_map.json\n",
      "Deleting older checkpoint [output/ExternalAttention/checkpoint-1530] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/ExternalAttention/checkpoint-2210\n",
      "Configuration saved in ./output/ExternalAttention/checkpoint-2210/config.json\n",
      "Model weights saved in ./output/ExternalAttention/checkpoint-2210/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/ExternalAttention/checkpoint-2210/tokenizer_config.json\n",
      "Special tokens file saved in ./output/ExternalAttention/checkpoint-2210/special_tokens_map.json\n",
      "Deleting older checkpoint [output/ExternalAttention/checkpoint-1700] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/ExternalAttention/checkpoint-2380\n",
      "Configuration saved in ./output/ExternalAttention/checkpoint-2380/config.json\n",
      "Model weights saved in ./output/ExternalAttention/checkpoint-2380/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/ExternalAttention/checkpoint-2380/tokenizer_config.json\n",
      "Special tokens file saved in ./output/ExternalAttention/checkpoint-2380/special_tokens_map.json\n",
      "Deleting older checkpoint [output/ExternalAttention/checkpoint-1870] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/ExternalAttention/checkpoint-2550\n",
      "Configuration saved in ./output/ExternalAttention/checkpoint-2550/config.json\n",
      "Model weights saved in ./output/ExternalAttention/checkpoint-2550/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/ExternalAttention/checkpoint-2550/tokenizer_config.json\n",
      "Special tokens file saved in ./output/ExternalAttention/checkpoint-2550/special_tokens_map.json\n",
      "Deleting older checkpoint [output/ExternalAttention/checkpoint-2040] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./output/ExternalAttention/checkpoint-2550 (score: 9.78715705871582).\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 409\n",
      "  Batch size = 6\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='69' max='69' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [69/69 00:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch import nn\n",
    "evals = {}\n",
    "for i,attn in enumerate(attn_li[:]):\n",
    "    not_allowed = ('AFT_FULL','EMSA')\n",
    "    print(attn)\n",
    "    if attn.__str__().startswith(not_allowed):\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    import os\n",
    "    model_name = attn.__str__().split('(')[0]\n",
    "    # model_name = model_checkpoint.split(\"/\")[-1] + model_name\n",
    "    saved_path = os.path.join('./output',model_name)\n",
    "    \n",
    "    args = Seq2SeqTrainingArguments(\n",
    "        saved_path,\n",
    "        evaluation_strategy = \"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        weight_decay=0.01,\n",
    "        save_total_limit=3,\n",
    "        num_train_epochs=15,\n",
    "        predict_with_generate=True,\n",
    "        fp16=True,\n",
    "        save_strategy='epoch',\n",
    "        load_best_model_at_end=True,\n",
    "    #     push_to_hub=True,\n",
    "    )\n",
    "\n",
    "    class tel2lin1(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(tel2lin1,self).__init__()\n",
    "            # self.tel = nn.TransformerEncoderLayer(d_model=768,nhead=3,batch_first=True)\n",
    "            self.tel = attn\n",
    "            self.lin = nn.Linear(in_features=768,out_features=50265, bias=False)\n",
    "            \n",
    "        def forward(self,x):\n",
    "            if self.tel.__str__().startswith('ExternalAttention'):\n",
    "                x = self.tel(x)\n",
    "            else:\n",
    "                x = self.tel(x,x,x)\n",
    "            x = self.lin(x)\n",
    "            return x\n",
    "    model.lm_head = tel2lin1()\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model,\n",
    "        args,\n",
    "        train_dataset=tr,\n",
    "        eval_dataset=val,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    \n",
    "    evals[f'{model_name}'] = trainer.evaluate(eval_dataset=te)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa490cb3-3748-44b9-84cb-4a4d263c0aaa",
   "metadata": {},
   "source": [
    "### Light 2L Attentions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a564b8-d678-4a39-b9e8-4b40d6906821",
   "metadata": {},
   "source": [
    "### 2L Attentions with Skip connection "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861a902e-078d-4134-8d39-9a91213b58b2",
   "metadata": {},
   "source": [
    "### 4L Attentions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846e2c52-4563-4f5b-87a6-f2fd014b33e4",
   "metadata": {},
   "source": [
    "### Etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfd22cbe-b702-4b43-b8f6-e918ffd221bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torch,copy\n",
    "class cons(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(cons,self).__init__()\n",
    "        self.front = nn.Sequential(\n",
    "            nn.Conv2d(1,32,3),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU() ,#batchnorm 2d ?\n",
    "            nn.Conv2d(32,16,2),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        sple = torch.rand(1,1,28,28)\n",
    "        front_out = self.front(sple).size()[-1]\n",
    "        self.body = nn.Sequential(\n",
    "            nn.Linear(front_out,256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(256,128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            # nn.Linear(128,10),\n",
    "        )\n",
    "        self.medium = nn.Sequential(\n",
    "            nn.Linear(128,64),\n",
    "            nn.Linear(64,128)\n",
    "        )\n",
    "        self.macro = [copy.deepcopy(self.medium) for _ in range(3)]\n",
    "        self.macro = nn.ModuleList(self.macro)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128,10),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        x = self.front(x)\n",
    "        x = self.body(x)\n",
    "        for i in range(len(self.macro)):\n",
    "            x = self.macro[i](x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "md = cons()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02c0c7d5-1a19-4dca-bb0b-3595481daa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "sli = [nn.Sequential(nn.Linear(128,64),nn.Linear(64,128)),\n",
    "       nn.Sequential(nn.Linear(128,64),nn.Linear(64,128)),\n",
    "       nn.Sequential(nn.Linear(128,64),nn.Linear(64,128))]\n",
    "mli = nn.ModuleList(sli)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c501c54a-d052-4aa1-830e-325a447c150b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c777196-5398-424b-8a7b-a13800d61956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "cons                                     --                        --\n",
       "├─ModuleList: 1-1                        --                        --\n",
       "├─Sequential: 1-2                        [4, 576]                  --\n",
       "│    └─Conv2d: 2-1                       [4, 32, 26, 26]           320\n",
       "│    └─MaxPool2d: 2-2                    [4, 32, 13, 13]           --\n",
       "│    └─BatchNorm2d: 2-3                  [4, 32, 13, 13]           64\n",
       "│    └─ReLU: 2-4                         [4, 32, 13, 13]           --\n",
       "│    └─Conv2d: 2-5                       [4, 16, 12, 12]           2,064\n",
       "│    └─MaxPool2d: 2-6                    [4, 16, 6, 6]             --\n",
       "│    └─BatchNorm2d: 2-7                  [4, 16, 6, 6]             32\n",
       "│    └─ReLU: 2-8                         [4, 16, 6, 6]             --\n",
       "│    └─Flatten: 2-9                      [4, 576]                  --\n",
       "├─Sequential: 1-3                        [4, 128]                  --\n",
       "│    └─Linear: 2-10                      [4, 256]                  147,712\n",
       "│    └─ReLU: 2-11                        [4, 256]                  --\n",
       "│    └─BatchNorm1d: 2-12                 [4, 256]                  512\n",
       "│    └─Dropout: 2-13                     [4, 256]                  --\n",
       "│    └─Linear: 2-14                      [4, 128]                  32,896\n",
       "│    └─BatchNorm1d: 2-15                 [4, 128]                  256\n",
       "│    └─ReLU: 2-16                        [4, 128]                  --\n",
       "├─ModuleList: 1-1                        --                        --\n",
       "│    └─Sequential: 2-17                  [4, 128]                  --\n",
       "│    │    └─Linear: 3-1                  [4, 64]                   8,256\n",
       "│    │    └─Linear: 3-2                  [4, 128]                  8,320\n",
       "│    └─Sequential: 2-18                  [4, 128]                  --\n",
       "│    │    └─Linear: 3-3                  [4, 64]                   8,256\n",
       "│    │    └─Linear: 3-4                  [4, 128]                  8,320\n",
       "│    └─Sequential: 2-19                  [4, 128]                  --\n",
       "│    │    └─Linear: 3-5                  [4, 64]                   8,256\n",
       "│    │    └─Linear: 3-6                  [4, 128]                  8,320\n",
       "├─Sequential: 1-4                        [4, 10]                   --\n",
       "│    └─Linear: 2-20                      [4, 10]                   1,290\n",
       "│    └─Softmax: 2-21                     [4, 10]                   --\n",
       "==========================================================================================\n",
       "Total params: 234,874\n",
       "Trainable params: 234,874\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 2.98\n",
       "==========================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 1.00\n",
       "Params size (MB): 0.94\n",
       "Estimated Total Size (MB): 1.95\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary as sm\n",
    "sm(md,(4,1,28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9bce4c5d-e68f-4d28-9aeb-df7398bbe241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cons(\n",
       "  (front): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(32, 16, kernel_size=(2, 2), stride=(1, 1))\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): ReLU()\n",
       "    (8): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (body): Sequential(\n",
       "    (0): Linear(in_features=576, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Dropout(p=0.2, inplace=False)\n",
       "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "  )\n",
       "  (medium): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (1): Linear(in_features=64, out_features=128, bias=True)\n",
       "  )\n",
       "  (macro): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): Linear(in_features=64, out_features=128, bias=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): Linear(in_features=64, out_features=128, bias=True)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): Linear(in_features=64, out_features=128, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=10, bias=True)\n",
       "    (1): Softmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7391b8-f426-4435-858b-188730c64271",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Eval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c1cb2d2-44cc-4598-8e1f-675d3e3665c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"attns.pickle\",\"wb\") as fw:\n",
    "    pickle.dump(evals, fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "981e4386-1059-464a-acfc-d32bc73f05ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "evals.update(evals_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbe3be0d-d17a-41a3-af24-a8c452a42002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_rouge1</th>\n",
       "      <th>eval_rouge2</th>\n",
       "      <th>eval_rougeL</th>\n",
       "      <th>eval_rougeLsum</th>\n",
       "      <th>eval_gen_len</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <th>epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>UFOAttention</th>\n",
       "      <td>6.409752</td>\n",
       "      <td>11.7499</td>\n",
       "      <td>1.4080</td>\n",
       "      <td>10.2117</td>\n",
       "      <td>10.2238</td>\n",
       "      <td>15.4963</td>\n",
       "      <td>39.6509</td>\n",
       "      <td>10.315</td>\n",
       "      <td>1.740</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MUSEAttention</th>\n",
       "      <td>6.341923</td>\n",
       "      <td>7.2341</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>6.3786</td>\n",
       "      <td>6.3928</td>\n",
       "      <td>16.3350</td>\n",
       "      <td>40.2946</td>\n",
       "      <td>10.150</td>\n",
       "      <td>1.712</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SimplifiedScaledDotProductAttention</th>\n",
       "      <td>6.524034</td>\n",
       "      <td>7.5712</td>\n",
       "      <td>0.6479</td>\n",
       "      <td>6.6932</td>\n",
       "      <td>6.7240</td>\n",
       "      <td>14.6406</td>\n",
       "      <td>39.1062</td>\n",
       "      <td>10.459</td>\n",
       "      <td>1.764</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ScaledDotProductAttention</th>\n",
       "      <td>7.004627</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>29.5214</td>\n",
       "      <td>13.854</td>\n",
       "      <td>2.337</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ExternalAttention</th>\n",
       "      <td>9.794738</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>29.9376</td>\n",
       "      <td>13.662</td>\n",
       "      <td>2.305</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base</th>\n",
       "      <td>8.350389</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>29.3628</td>\n",
       "      <td>13.929</td>\n",
       "      <td>2.350</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     eval_loss  eval_rouge1  eval_rouge2  \\\n",
       "UFOAttention                          6.409752      11.7499       1.4080   \n",
       "MUSEAttention                         6.341923       7.2341       0.0000   \n",
       "SimplifiedScaledDotProductAttention   6.524034       7.5712       0.6479   \n",
       "ScaledDotProductAttention             7.004627       0.0000       0.0000   \n",
       "ExternalAttention                     9.794738       0.0000       0.0000   \n",
       "base                                  8.350389       0.0000       0.0000   \n",
       "\n",
       "                                     eval_rougeL  eval_rougeLsum  \\\n",
       "UFOAttention                             10.2117         10.2238   \n",
       "MUSEAttention                             6.3786          6.3928   \n",
       "SimplifiedScaledDotProductAttention       6.6932          6.7240   \n",
       "ScaledDotProductAttention                 0.0000          0.0000   \n",
       "ExternalAttention                         0.0000          0.0000   \n",
       "base                                      0.0000          0.0000   \n",
       "\n",
       "                                     eval_gen_len  eval_runtime  \\\n",
       "UFOAttention                              15.4963       39.6509   \n",
       "MUSEAttention                             16.3350       40.2946   \n",
       "SimplifiedScaledDotProductAttention       14.6406       39.1062   \n",
       "ScaledDotProductAttention                  4.0000       29.5214   \n",
       "ExternalAttention                          5.0000       29.9376   \n",
       "base                                       2.0000       29.3628   \n",
       "\n",
       "                                     eval_samples_per_second  \\\n",
       "UFOAttention                                          10.315   \n",
       "MUSEAttention                                         10.150   \n",
       "SimplifiedScaledDotProductAttention                   10.459   \n",
       "ScaledDotProductAttention                             13.854   \n",
       "ExternalAttention                                     13.662   \n",
       "base                                                  13.929   \n",
       "\n",
       "                                     eval_steps_per_second  epoch  \n",
       "UFOAttention                                         1.740   15.0  \n",
       "MUSEAttention                                        1.712   15.0  \n",
       "SimplifiedScaledDotProductAttention                  1.764   15.0  \n",
       "ScaledDotProductAttention                            2.337   15.0  \n",
       "ExternalAttention                                    2.305   15.0  \n",
       "base                                                 2.350   15.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(evals).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be11ea49-1f71-455f-b8b8-a496c69e1251",
   "metadata": {},
   "source": [
    "### basic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b1a89a0-bed1-4a0b-b81b-893ee63c755a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using amp fp16 backend\n",
      "The following columns in the training set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running training *****\n",
      "  Num examples = 1020\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 3\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 6\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2550\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2550' max='2550' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2550/2550 30:31, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>9.611067</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>9.436999</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>9.370300</td>\n",
       "      <td>9.270031</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>9.370300</td>\n",
       "      <td>9.115933</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>9.370300</td>\n",
       "      <td>8.973625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>8.827900</td>\n",
       "      <td>8.845672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>8.827900</td>\n",
       "      <td>8.732598</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>8.827900</td>\n",
       "      <td>8.635551</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>8.406700</td>\n",
       "      <td>8.551472</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>8.406700</td>\n",
       "      <td>8.482576</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>8.406700</td>\n",
       "      <td>8.425470</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>8.135300</td>\n",
       "      <td>8.383299</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>8.135300</td>\n",
       "      <td>8.353939</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>8.135300</td>\n",
       "      <td>8.336150</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>7.988500</td>\n",
       "      <td>8.330002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/base/checkpoint-170\n",
      "Configuration saved in ./output/base/checkpoint-170/config.json\n",
      "Model weights saved in ./output/base/checkpoint-170/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/base/checkpoint-170/tokenizer_config.json\n",
      "Special tokens file saved in ./output/base/checkpoint-170/special_tokens_map.json\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/base/checkpoint-340\n",
      "Configuration saved in ./output/base/checkpoint-340/config.json\n",
      "Model weights saved in ./output/base/checkpoint-340/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/base/checkpoint-340/tokenizer_config.json\n",
      "Special tokens file saved in ./output/base/checkpoint-340/special_tokens_map.json\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/base/checkpoint-510\n",
      "Configuration saved in ./output/base/checkpoint-510/config.json\n",
      "Model weights saved in ./output/base/checkpoint-510/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/base/checkpoint-510/tokenizer_config.json\n",
      "Special tokens file saved in ./output/base/checkpoint-510/special_tokens_map.json\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/base/checkpoint-680\n",
      "Configuration saved in ./output/base/checkpoint-680/config.json\n",
      "Model weights saved in ./output/base/checkpoint-680/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/base/checkpoint-680/tokenizer_config.json\n",
      "Special tokens file saved in ./output/base/checkpoint-680/special_tokens_map.json\n",
      "Deleting older checkpoint [output/base/checkpoint-170] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/base/checkpoint-850\n",
      "Configuration saved in ./output/base/checkpoint-850/config.json\n",
      "Model weights saved in ./output/base/checkpoint-850/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/base/checkpoint-850/tokenizer_config.json\n",
      "Special tokens file saved in ./output/base/checkpoint-850/special_tokens_map.json\n",
      "Deleting older checkpoint [output/base/checkpoint-340] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/base/checkpoint-1020\n",
      "Configuration saved in ./output/base/checkpoint-1020/config.json\n",
      "Model weights saved in ./output/base/checkpoint-1020/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/base/checkpoint-1020/tokenizer_config.json\n",
      "Special tokens file saved in ./output/base/checkpoint-1020/special_tokens_map.json\n",
      "Deleting older checkpoint [output/base/checkpoint-510] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/base/checkpoint-1190\n",
      "Configuration saved in ./output/base/checkpoint-1190/config.json\n",
      "Model weights saved in ./output/base/checkpoint-1190/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/base/checkpoint-1190/tokenizer_config.json\n",
      "Special tokens file saved in ./output/base/checkpoint-1190/special_tokens_map.json\n",
      "Deleting older checkpoint [output/base/checkpoint-680] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/base/checkpoint-1360\n",
      "Configuration saved in ./output/base/checkpoint-1360/config.json\n",
      "Model weights saved in ./output/base/checkpoint-1360/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/base/checkpoint-1360/tokenizer_config.json\n",
      "Special tokens file saved in ./output/base/checkpoint-1360/special_tokens_map.json\n",
      "Deleting older checkpoint [output/base/checkpoint-850] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/base/checkpoint-1530\n",
      "Configuration saved in ./output/base/checkpoint-1530/config.json\n",
      "Model weights saved in ./output/base/checkpoint-1530/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/base/checkpoint-1530/tokenizer_config.json\n",
      "Special tokens file saved in ./output/base/checkpoint-1530/special_tokens_map.json\n",
      "Deleting older checkpoint [output/base/checkpoint-1020] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/base/checkpoint-1700\n",
      "Configuration saved in ./output/base/checkpoint-1700/config.json\n",
      "Model weights saved in ./output/base/checkpoint-1700/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/base/checkpoint-1700/tokenizer_config.json\n",
      "Special tokens file saved in ./output/base/checkpoint-1700/special_tokens_map.json\n",
      "Deleting older checkpoint [output/base/checkpoint-1190] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/base/checkpoint-1870\n",
      "Configuration saved in ./output/base/checkpoint-1870/config.json\n",
      "Model weights saved in ./output/base/checkpoint-1870/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/base/checkpoint-1870/tokenizer_config.json\n",
      "Special tokens file saved in ./output/base/checkpoint-1870/special_tokens_map.json\n",
      "Deleting older checkpoint [output/base/checkpoint-1360] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/base/checkpoint-2040\n",
      "Configuration saved in ./output/base/checkpoint-2040/config.json\n",
      "Model weights saved in ./output/base/checkpoint-2040/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/base/checkpoint-2040/tokenizer_config.json\n",
      "Special tokens file saved in ./output/base/checkpoint-2040/special_tokens_map.json\n",
      "Deleting older checkpoint [output/base/checkpoint-1530] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/base/checkpoint-2210\n",
      "Configuration saved in ./output/base/checkpoint-2210/config.json\n",
      "Model weights saved in ./output/base/checkpoint-2210/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/base/checkpoint-2210/tokenizer_config.json\n",
      "Special tokens file saved in ./output/base/checkpoint-2210/special_tokens_map.json\n",
      "Deleting older checkpoint [output/base/checkpoint-1700] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/base/checkpoint-2380\n",
      "Configuration saved in ./output/base/checkpoint-2380/config.json\n",
      "Model weights saved in ./output/base/checkpoint-2380/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/base/checkpoint-2380/tokenizer_config.json\n",
      "Special tokens file saved in ./output/base/checkpoint-2380/special_tokens_map.json\n",
      "Deleting older checkpoint [output/base/checkpoint-1870] due to args.save_total_limit\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./output/base/checkpoint-2550\n",
      "Configuration saved in ./output/base/checkpoint-2550/config.json\n",
      "Model weights saved in ./output/base/checkpoint-2550/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/base/checkpoint-2550/tokenizer_config.json\n",
      "Special tokens file saved in ./output/base/checkpoint-2550/special_tokens_map.json\n",
      "Deleting older checkpoint [output/base/checkpoint-2040] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./output/base/checkpoint-2550 (score: 8.330001831054688).\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 409\n",
      "  Batch size = 6\n",
      "/home/yp/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='69' max='69' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [69/69 00:28]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch import nn\n",
    "evals_base = {}\n",
    "for i,attn in enumerate(['base']):\n",
    "\n",
    "    import os\n",
    "    model_name = attn\n",
    "    # model_name = model_checkpoint.split(\"/\")[-1] + model_name\n",
    "    saved_path = os.path.join('./output',model_name)\n",
    "    \n",
    "    args = Seq2SeqTrainingArguments(\n",
    "        saved_path,\n",
    "        evaluation_strategy = \"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        weight_decay=0.01,\n",
    "        save_total_limit=3,\n",
    "        num_train_epochs=15,\n",
    "        predict_with_generate=True,\n",
    "        fp16=True,\n",
    "        save_strategy='epoch',\n",
    "        load_best_model_at_end=True,\n",
    "    #     push_to_hub=True,\n",
    "    )\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model,\n",
    "        args,\n",
    "        train_dataset=tr,\n",
    "        eval_dataset=val,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    \n",
    "    evals[f'{model_name}'] = trainer.evaluate(eval_dataset=te)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
